{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# date: 04.29.24"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31X412tX5w9Y",
        "outputId": "2034fd57-e435-4cc9-9af5-bcc2c75217da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170498071/170498071 [00:13<00:00, 12885175.64it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 160MB/s]\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([50000, 512])\n",
            "Epoch 1, Batch 1, Loss: 0.05045027285814285\n",
            "Epoch 1, Batch 101, Loss: 0.03767799213528633\n",
            "Epoch 1, Batch 201, Loss: 0.036080148071050644\n",
            "Epoch 1, Batch 301, Loss: 0.031091976910829544\n",
            "Epoch 1, Batch 401, Loss: 0.03137672692537308\n",
            "Epoch 1, Batch 501, Loss: 0.0308985598385334\n",
            "Epoch 1, Batch 601, Loss: 0.02739039622247219\n",
            "Epoch 1, Batch 701, Loss: 0.026187393814325333\n",
            "Epoch 1, Batch 801, Loss: 0.03371585160493851\n",
            "Epoch 1, Batch 901, Loss: 0.025045275688171387\n",
            "Epoch 1, Batch 1001, Loss: 0.028023332357406616\n",
            "Epoch 1, Batch 1101, Loss: 0.029338065534830093\n",
            "Epoch 1, Batch 1201, Loss: 0.029069507494568825\n",
            "Epoch 1, Batch 1301, Loss: 0.026306474581360817\n",
            "Epoch 1, Batch 1401, Loss: 0.026881495490670204\n",
            "Epoch 1, Batch 1501, Loss: 0.025451797991991043\n",
            "Epoch 2, Batch 1, Loss: 0.02591143548488617\n",
            "Epoch 2, Batch 101, Loss: 0.023996198549866676\n",
            "Epoch 2, Batch 201, Loss: 0.029319990426301956\n",
            "Epoch 2, Batch 301, Loss: 0.024861324578523636\n",
            "Epoch 2, Batch 401, Loss: 0.025615502148866653\n",
            "Epoch 2, Batch 501, Loss: 0.02670791558921337\n",
            "Epoch 2, Batch 601, Loss: 0.023218534886837006\n",
            "Epoch 2, Batch 701, Loss: 0.022767474874854088\n",
            "Epoch 2, Batch 801, Loss: 0.03245697543025017\n",
            "Epoch 2, Batch 901, Loss: 0.023151256144046783\n",
            "Epoch 2, Batch 1001, Loss: 0.026193568482995033\n",
            "Epoch 2, Batch 1101, Loss: 0.0279897041618824\n",
            "Epoch 2, Batch 1201, Loss: 0.02717067115008831\n",
            "Epoch 2, Batch 1301, Loss: 0.02553589455783367\n",
            "Epoch 2, Batch 1401, Loss: 0.026179114356637\n",
            "Epoch 2, Batch 1501, Loss: 0.02386392466723919\n",
            "Epoch 3, Batch 1, Loss: 0.02518642321228981\n",
            "Epoch 3, Batch 101, Loss: 0.023158622905611992\n",
            "Epoch 3, Batch 201, Loss: 0.028969889506697655\n",
            "Epoch 3, Batch 301, Loss: 0.024136876687407494\n",
            "Epoch 3, Batch 401, Loss: 0.024746930226683617\n",
            "Epoch 3, Batch 501, Loss: 0.025982016697525978\n",
            "Epoch 3, Batch 601, Loss: 0.022349532693624496\n",
            "Epoch 3, Batch 701, Loss: 0.022120095789432526\n",
            "Epoch 3, Batch 801, Loss: 0.032025374472141266\n",
            "Epoch 3, Batch 901, Loss: 0.02268807776272297\n",
            "Epoch 3, Batch 1001, Loss: 0.025779301300644875\n",
            "Epoch 3, Batch 1101, Loss: 0.027560798451304436\n",
            "Epoch 3, Batch 1201, Loss: 0.02635391615331173\n",
            "Epoch 3, Batch 1301, Loss: 0.025213349610567093\n",
            "Epoch 3, Batch 1401, Loss: 0.025960370898246765\n",
            "Epoch 3, Batch 1501, Loss: 0.023327279835939407\n",
            "Epoch 4, Batch 1, Loss: 0.024872882291674614\n",
            "Epoch 4, Batch 101, Loss: 0.022960081696510315\n",
            "Epoch 4, Batch 201, Loss: 0.02885628305375576\n",
            "Epoch 4, Batch 301, Loss: 0.02382533624768257\n",
            "Epoch 4, Batch 401, Loss: 0.02442433498799801\n",
            "Epoch 4, Batch 501, Loss: 0.025661399587988853\n",
            "Epoch 4, Batch 601, Loss: 0.02199743129312992\n",
            "Epoch 4, Batch 701, Loss: 0.02179320901632309\n",
            "Epoch 4, Batch 801, Loss: 0.03179193660616875\n",
            "Epoch 4, Batch 901, Loss: 0.022467896342277527\n",
            "Epoch 4, Batch 1001, Loss: 0.025605855509638786\n",
            "Epoch 4, Batch 1101, Loss: 0.02733038179576397\n",
            "Epoch 4, Batch 1201, Loss: 0.025888532400131226\n",
            "Epoch 4, Batch 1301, Loss: 0.024994034320116043\n",
            "Epoch 4, Batch 1401, Loss: 0.025810766965150833\n",
            "Epoch 4, Batch 1501, Loss: 0.023090390488505363\n",
            "Epoch 5, Batch 1, Loss: 0.024670429527759552\n",
            "Epoch 5, Batch 101, Loss: 0.022912994027137756\n",
            "Epoch 5, Batch 201, Loss: 0.028767472133040428\n",
            "Epoch 5, Batch 301, Loss: 0.02364586666226387\n",
            "Epoch 5, Batch 401, Loss: 0.02422400377690792\n",
            "Epoch 5, Batch 501, Loss: 0.025453412905335426\n",
            "Epoch 5, Batch 601, Loss: 0.02181924320757389\n",
            "Epoch 5, Batch 701, Loss: 0.02157631143927574\n",
            "Epoch 5, Batch 801, Loss: 0.03163979575037956\n",
            "Epoch 5, Batch 901, Loss: 0.02235116809606552\n",
            "Epoch 5, Batch 1001, Loss: 0.025504035875201225\n",
            "Epoch 5, Batch 1101, Loss: 0.027183303609490395\n",
            "Epoch 5, Batch 1201, Loss: 0.025586027652025223\n",
            "Epoch 5, Batch 1301, Loss: 0.02483534626662731\n",
            "Epoch 5, Batch 1401, Loss: 0.025690054520964622\n",
            "Epoch 5, Batch 1501, Loss: 0.0229655634611845\n",
            "Epoch 6, Batch 1, Loss: 0.02452491782605648\n",
            "Epoch 6, Batch 101, Loss: 0.02291252091526985\n",
            "Epoch 6, Batch 201, Loss: 0.028689200058579445\n",
            "Epoch 6, Batch 301, Loss: 0.023529373109340668\n",
            "Epoch 6, Batch 401, Loss: 0.024069925770163536\n",
            "Epoch 6, Batch 501, Loss: 0.02529744803905487\n",
            "Epoch 6, Batch 601, Loss: 0.021716130897402763\n",
            "Epoch 6, Batch 701, Loss: 0.021418197080492973\n",
            "Epoch 6, Batch 801, Loss: 0.031528111547231674\n",
            "Epoch 6, Batch 901, Loss: 0.022290615364909172\n",
            "Epoch 6, Batch 1001, Loss: 0.02543109469115734\n",
            "Epoch 6, Batch 1101, Loss: 0.027081510052084923\n",
            "Epoch 6, Batch 1201, Loss: 0.025371631607413292\n",
            "Epoch 6, Batch 1301, Loss: 0.024718157947063446\n",
            "Epoch 6, Batch 1401, Loss: 0.02559124305844307\n",
            "Epoch 6, Batch 1501, Loss: 0.0228895153850317\n",
            "Epoch 7, Batch 1, Loss: 0.024413926526904106\n",
            "Epoch 7, Batch 101, Loss: 0.022926418110728264\n",
            "Epoch 7, Batch 201, Loss: 0.028621001169085503\n",
            "Epoch 7, Batch 301, Loss: 0.023447884246706963\n",
            "Epoch 7, Batch 401, Loss: 0.02394341304898262\n",
            "Epoch 7, Batch 501, Loss: 0.02517297863960266\n",
            "Epoch 7, Batch 601, Loss: 0.021650897338986397\n",
            "Epoch 7, Batch 701, Loss: 0.02129797451198101\n",
            "Epoch 7, Batch 801, Loss: 0.031439702957868576\n",
            "Epoch 7, Batch 901, Loss: 0.02226302959024906\n",
            "Epoch 7, Batch 1001, Loss: 0.025372589007019997\n",
            "Epoch 7, Batch 1101, Loss: 0.027007419615983963\n",
            "Epoch 7, Batch 1201, Loss: 0.025209737941622734\n",
            "Epoch 7, Batch 1301, Loss: 0.024629918858408928\n",
            "Epoch 7, Batch 1401, Loss: 0.025510793551802635\n",
            "Epoch 7, Batch 1501, Loss: 0.022837426513433456\n",
            "Epoch 8, Batch 1, Loss: 0.024325771257281303\n",
            "Epoch 8, Batch 101, Loss: 0.02294357493519783\n",
            "Epoch 8, Batch 201, Loss: 0.028562569990754128\n",
            "Epoch 8, Batch 301, Loss: 0.023387759923934937\n",
            "Epoch 8, Batch 401, Loss: 0.023837530985474586\n",
            "Epoch 8, Batch 501, Loss: 0.025070084258913994\n",
            "Epoch 8, Batch 601, Loss: 0.021606966853141785\n",
            "Epoch 8, Batch 701, Loss: 0.021204447373747826\n",
            "Epoch 8, Batch 801, Loss: 0.03136633709073067\n",
            "Epoch 8, Batch 901, Loss: 0.02225521020591259\n",
            "Epoch 8, Batch 1001, Loss: 0.02532261051237583\n",
            "Epoch 8, Batch 1101, Loss: 0.026951510459184647\n",
            "Epoch 8, Batch 1201, Loss: 0.02508152835071087\n",
            "Epoch 8, Batch 1301, Loss: 0.024562088772654533\n",
            "Epoch 8, Batch 1401, Loss: 0.025445623323321342\n",
            "Epoch 8, Batch 1501, Loss: 0.022798387333750725\n",
            "Epoch 9, Batch 1, Loss: 0.024253740906715393\n",
            "Epoch 9, Batch 101, Loss: 0.02296014502644539\n",
            "Epoch 9, Batch 201, Loss: 0.028513092547655106\n",
            "Epoch 9, Batch 301, Loss: 0.023341698572039604\n",
            "Epoch 9, Batch 401, Loss: 0.023748300969600677\n",
            "Epoch 9, Batch 501, Loss: 0.024982957169413567\n",
            "Epoch 9, Batch 601, Loss: 0.021575940772891045\n",
            "Epoch 9, Batch 701, Loss: 0.021130600944161415\n",
            "Epoch 9, Batch 801, Loss: 0.031303614377975464\n",
            "Epoch 9, Batch 901, Loss: 0.02225923165678978\n",
            "Epoch 9, Batch 1001, Loss: 0.02527838945388794\n",
            "Epoch 9, Batch 1101, Loss: 0.026908129453659058\n",
            "Epoch 9, Batch 1201, Loss: 0.0249763373285532\n",
            "Epoch 9, Batch 1301, Loss: 0.02450886182487011\n",
            "Epoch 9, Batch 1401, Loss: 0.025392979383468628\n",
            "Epoch 9, Batch 1501, Loss: 0.02276717498898506\n",
            "Epoch 10, Batch 1, Loss: 0.024193687364459038\n",
            "Epoch 10, Batch 101, Loss: 0.022974969819188118\n",
            "Epoch 10, Batch 201, Loss: 0.02847149409353733\n",
            "Epoch 10, Batch 301, Loss: 0.023305466398596764\n",
            "Epoch 10, Batch 401, Loss: 0.023672791197896004\n",
            "Epoch 10, Batch 501, Loss: 0.02490784041583538\n",
            "Epoch 10, Batch 601, Loss: 0.02155315689742565\n",
            "Epoch 10, Batch 701, Loss: 0.021071622148156166\n",
            "Epoch 10, Batch 801, Loss: 0.03124896250665188\n",
            "Epoch 10, Batch 901, Loss: 0.022270219400525093\n",
            "Epoch 10, Batch 1001, Loss: 0.025238487869501114\n",
            "Epoch 10, Batch 1101, Loss: 0.026873691007494926\n",
            "Epoch 10, Batch 1201, Loss: 0.02488773502409458\n",
            "Epoch 10, Batch 1301, Loss: 0.02446626126766205\n",
            "Epoch 10, Batch 1401, Loss: 0.025350509211421013\n",
            "Epoch 10, Batch 1501, Loss: 0.02274107187986374\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.models import resnet18,resnet50,vgg16\n",
        "import numpy as np\n",
        "\n",
        "# 1. Load CIFAR-10 dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False, num_workers=2)\n",
        "\n",
        "num_classes = 10\n",
        "\n",
        "def one_hot_encode(labels, num_classes):\n",
        "    '''\n",
        "    Convert an array of labels to a one-hot encoded matrix.\n",
        "\n",
        "    Args:\n",
        "    - labels (Tensor): A 1D tensor containing the labels.\n",
        "    - num_classes (int): The number of classes.\n",
        "\n",
        "    Returns:\n",
        "    - Tensor: A matrix where each row is the one-hot encoded version of the corresponding label.\n",
        "    '''\n",
        "    # Create an empty tensor filled with zeros\n",
        "    device = labels.device  # Get the device of the labels tensor\n",
        "    one_hot = torch.zeros(labels.size(0), num_classes, device=device)\n",
        "    # one_hot = torch.zeros(labels.size(0), num_classes)\n",
        "\n",
        "    # Fill the locations corresponding to the labels with ones\n",
        "    one_hot.scatter_(1, labels.unsqueeze(1), 1)\n",
        "\n",
        "    return one_hot\n",
        "\n",
        "# 2. Feature extraction using pretrained model\n",
        "class FeatureExtractor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FeatureExtractor, self).__init__()\n",
        "        model = resnet18(pretrained=True)\n",
        "        self.features = nn.Sequential(*list(model.children())[:-1])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return x\n",
        "\n",
        "def l2_regularization(model):\n",
        "    l2_reg = 0.0\n",
        "    for param in model.parameters():\n",
        "        l2_reg += torch.norm(param, p=2)\n",
        "    return l2_reg\n",
        "\n",
        "#  def additional_regularization(b, w):\n",
        "    # return torch.dot(b, w.view(-1))\n",
        "\n",
        "def normalize_columns(tensor):\n",
        "    \"\"\"\n",
        "    Normalize the columns of the input tensor such that the diagonal entries\n",
        "    of tensor.T @ tensor are 1.\n",
        "    \"\"\"\n",
        "    norms = tensor.norm(p=2, dim=0)  # Compute L2-norm for each column\n",
        "    return tensor / norms\n",
        "\n",
        "def bound_norm(data, upper_bound):\n",
        "    norms = data.norm(dim=1, keepdim=True)\n",
        "    scale = upper_bound / norms\n",
        "    scale[scale > 1] = 1  # Only scale vectors with norm > upper_bound\n",
        "    return data * scale\n",
        "\n",
        "feature_extractor = FeatureExtractor().cuda()\n",
        "feat_dim = 512\n",
        "\n",
        "# Extract features for entire training set\n",
        "train_features = []\n",
        "train_labels_list = []\n",
        "for inputs, labels in trainloader:\n",
        "    inputs = inputs.cuda()\n",
        "    features = feature_extractor(inputs).detach().cpu()\n",
        "    train_features.append(features)\n",
        "    train_labels_list.append(labels)\n",
        "\n",
        "train_features = torch.cat(train_features, dim=0)\n",
        "train_labels = torch.cat(train_labels_list, dim=0)\n",
        "\n",
        "print(train_features.shape)\n",
        "\n",
        "# train_features =  normalize_columns(train_features)\n",
        "train_features =  bound_norm(train_features,1)\n",
        "\n",
        "# train_feature_mean = train_features.mean(dim=0, keepdim=True)\n",
        "# train_feature_variance = train_features.var(dim=0, keepdim=True)\n",
        "\n",
        "# print(train_feature_mean.shape)\n",
        "# # print(train_feature_variance.shape)\n",
        "\n",
        "# train_features = (train_features-train_feature_mean)/train_feature_variance\n",
        "\n",
        "# 3. Linear classifier without bias\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.fc = nn.Linear(input_dim, output_dim, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "classifier = Classifier(feat_dim, num_classes).cuda()\n",
        "\n",
        "# 4. Quadratic loss\n",
        "# def quadratic_loss(outputs, targets, classifier, mu):\n",
        "#     return torch.mean(0.5*(outputs - targets)**2 + 0.5*mu*torch.norm((classifier.fc.weight))**2)\n",
        "\n",
        "def quadratic_loss(outputs, targets):\n",
        "    return torch.mean(0.5*(outputs - targets)**2)\n",
        "\n",
        "optimizer = optim.Adam(classifier.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "classifier_r = Classifier(feat_dim, num_classes).cuda()\n",
        "optimizer_r = optim.Adam(classifier_r.parameters(), lr=0.001)\n",
        "\n",
        "mu = 0.000001\n",
        "psi = 0.01\n",
        "\n",
        "# b = torch.from_numpy(np.random.exponential(size=512)).float().cuda()\n",
        "\n",
        "# 5. Train the classifier using the pre-extracted features\n",
        "for epoch in range(10):\n",
        "    for i in range(0, len(train_features), 32):\n",
        "        inputs = train_features[i:i+32].cuda()\n",
        "        labels = train_labels[i:i+32].cuda()\n",
        "        # labels_onehot = torch.zeros(labels.size(0), num_classes).cuda().scatter_(1, labels.view(-1, 1), 1)\n",
        "        labels_onehot = one_hot_encode(labels.cuda(), num_classes)\n",
        "\n",
        "        outputs = classifier(inputs)\n",
        "        # loss = quadratic_loss(outputs, labels_onehot, classifier, mu) + psi* additional_regularization(b, classifier.fc.weight)\n",
        "        loss = quadratic_loss(outputs, labels_onehot)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if i % 3200 == 0:  # Print training loss every 100 batches\n",
        "            print(f\"Epoch {epoch + 1}, Batch {i // 32 + 1}, Loss: {loss.item()}\")\n",
        "\n",
        "# 6. Evaluate the classifier on the test set using pre-extracted features\n",
        "classifier.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "test_features = []\n",
        "test_labels_list = []\n",
        "for inputs, labels in testloader:\n",
        "    inputs = inputs.cuda()\n",
        "    features = feature_extractor(inputs).detach().cpu()\n",
        "    test_features.append(features)\n",
        "    test_labels_list.append(labels)\n",
        "\n",
        "test_features = torch.cat(test_features, dim=0)\n",
        "test_labels = torch.cat(test_labels_list, dim=0)\n",
        "\n",
        "# test_features = (test_features-train_feature_mean)/train_feature_variance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLGZWy019N5F",
        "outputId": "7a384893-85a2-48ab-bb13-b2a1ec3df86d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([50000, 512])\n",
            "<class 'torch.Tensor'>\n"
          ]
        }
      ],
      "source": [
        "print(train_features.size())\n",
        "print(type(train_features))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IcmqXe6z6mOx",
        "outputId": "eb4739e2-9380-4429-c277-4c04500788eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'torch.Tensor'>\n",
            "torch.Size([50000])\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader, random_split\n",
        "# from torch.utils.data import Dataset, Subset\n",
        "\n",
        "# remain_size = int(0.6 * len(trainset))\n",
        "# forget_size = len(trainset) - remain_size\n",
        "\n",
        "\n",
        "forget_class = 5\n",
        "\n",
        "forget_indices = torch.where(train_labels == forget_class)[0]\n",
        "\n",
        "# # Split the dataset into two parts: class 5 and the remaining classes\n",
        "# remaining_indices = torch.where(train_labels != forget_class)[0]\n",
        "\n",
        "# forget_indices = torch.tensor(forget_indices)\n",
        "# remaining_indices = torch.tensor(remaining_indices)\n",
        "\n",
        "\n",
        "# print(trainset)\n",
        "# print(forget_indices)\n",
        "\n",
        "# remaining_features, forget_features =  random_split(train_features, [remain_size, forget_size])\n",
        "# remaining_indices = remaining_features.indices\n",
        "# remaining_labels = train_labels[remaining_indices]\n",
        "# forget_indices = forget_features.indices\n",
        "# forget_labels = train_labels[forget_indices]\n",
        "# print(type(remaining_features))\n",
        "\n",
        "num_forget_samples = int(0.8 * len(forget_indices))\n",
        "\n",
        "# Randomly select indices for the forget set\n",
        "forget_indices_sub = torch.randperm(len(forget_indices))[:num_forget_samples]\n",
        "\n",
        "# Remaining indices for class 5 (10%)\n",
        "remaining_indices_class5 = forget_indices[list(set(range(len(forget_indices))) - set(forget_indices_sub))]\n",
        "# remaining_indices_class5 = list(set(range(len(forget_indices))) - set(forget_indices_sub))\n",
        "\n",
        "forget_indices = forget_indices[forget_indices_sub]\n",
        "\n",
        "\n",
        "\n",
        "# Remaining indices for other classes\n",
        "remaining_indices_other_classes = torch.where(train_labels != forget_class)[0]\n",
        "\n",
        "# Concatenate indices for the remaining set\n",
        "remaining_indices = torch.cat((remaining_indices_class5, remaining_indices_other_classes))\n",
        "\n",
        "\n",
        "forget_features = train_features[forget_indices]\n",
        "remaining_features = train_features[remaining_indices]\n",
        "\n",
        "forget_labels = train_labels[forget_indices]\n",
        "remaining_labels = train_labels[remaining_indices]\n",
        "\n",
        "\n",
        "tensors_rf = [remaining_features[i] for i in range(len(remaining_features))]\n",
        "tensors_rl = [remaining_labels[i] for i in range(len(remaining_labels))]\n",
        "remaining_features = torch.stack(tensors_rf)\n",
        "remaining_labels = torch.stack(tensors_rl)\n",
        "print(type(remaining_labels))\n",
        "print(remaining_labels.size())\n",
        "\n",
        "tensors_ff = [forget_features[i] for i in range(len(forget_features))]\n",
        "tensors_fl = [forget_labels[i] for i in range(len(forget_labels))]\n",
        "forget_features = torch.stack(tensors_ff)\n",
        "forget_labels = torch.stack(tensors_fl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGK82303KmZG"
      },
      "outputs": [],
      "source": [
        "def extract_features(model, features):\n",
        "    model.eval()\n",
        "    out = []\n",
        "    labels = []\n",
        "    with torch.no_grad():\n",
        "      for i in range(0, len(features), 32):\n",
        "        inputs = features[i:i+32].cuda()\n",
        "        outputs = model(inputs)\n",
        "        # print(outputs)\n",
        "        # outputs.append(outputs.cpu().numpy())\n",
        "        out.append(outputs.cpu().numpy())\n",
        "    return out\n",
        "\n",
        "class BinaryClassifier(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(BinaryClassifier, self).__init__()\n",
        "        self.fc = nn.Linear(input_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "def train_binary_classifier(model, D_r_features, D_r_labels, D_test_features, D_test_labels):\n",
        "    # Create binary labels for binary classification\n",
        "    D_r_binary_labels = np.ones_like(D_r_labels)\n",
        "    D_test_binary_labels = np.zeros_like(D_test_labels)\n",
        "\n",
        "    # Use model outputs on D_r as features for training\n",
        "    # binary_train_features = D_r_features\n",
        "    # binary_train_labels = D_r_binary_labels\n",
        "\n",
        "    binary_train_features = np.vstack((D_r_features, D_test_features))\n",
        "    binary_train_labels = np.concatenate((D_r_binary_labels, D_test_binary_labels)).flatten()\n",
        "\n",
        "    # Move binary classifier and data to GPU\n",
        "    binary_train_features_tensor = torch.tensor(binary_train_features, dtype=torch.float32).cuda()\n",
        "    binary_train_labels_tensor = torch.tensor(binary_train_labels, dtype=torch.float32).cuda()\n",
        "\n",
        "    # Initialize binary classifier\n",
        "    binary_classifier = BinaryClassifier(binary_train_features.shape[1]).cuda()\n",
        "\n",
        "    # Define binary cross-entropy loss and optimizer\n",
        "    binary_criterion = nn.BCEWithLogitsLoss()\n",
        "    binary_optimizer = optim.SGD(binary_classifier.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "    # Training loop for binary classifier\n",
        "    num_binary_epochs = 50  # You can adjust the number of epochs\n",
        "    for epoch in range(num_binary_epochs):\n",
        "        binary_classifier.train()\n",
        "        binary_optimizer.zero_grad()\n",
        "        binary_outputs = binary_classifier(binary_train_features_tensor)\n",
        "        binary_loss = binary_criterion(binary_outputs.squeeze(dim=1), binary_train_labels_tensor)\n",
        "        binary_loss.backward()\n",
        "        binary_optimizer.step()\n",
        "\n",
        "    return binary_classifier, binary_train_features_tensor, binary_train_labels_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVDM5gRt9Poh",
        "outputId": "05d7dbba-9e3c-4bad-9630-d7b88a8ec42e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(0.0247, device='cuda:0', grad_fn=<MeanBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# for i in range(0, len(forget_features)):\n",
        "forget_inputs = forget_features.cuda()\n",
        "pred_forget_labels = forget_labels.cuda()\n",
        "        # labels_onehot = torch.zeros(labels.size(0), num_classes).cuda().scatter_(1, labels.view(-1, 1), 1)\n",
        "forget_labels_onehot = one_hot_encode(forget_labels.cuda(), num_classes)\n",
        "forget_outputs = classifier(forget_inputs)\n",
        "# loss_f = quadratic_loss(forget_outputs, forget_labels_onehot, classifier, mu) + psi* additional_regularization(b, classifier.fc.weight)\n",
        "loss_f = quadratic_loss(forget_outputs, forget_labels_onehot)\n",
        "print(loss_f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tp-D5mErjy4k",
        "outputId": "cd89de27-1e61-433e-c504-2131a4acd80d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy on the test set using original model: 77 %\n",
            "Accuracy on the remaining train set using original model: 77 %\n",
            "Accuracy on the forget train set using original model: 73 %\n"
          ]
        }
      ],
      "source": [
        "classifier.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i in range(0, len(test_features), 32):\n",
        "        inputs = test_features[i:i+32].cuda()\n",
        "        labels = test_labels[i:i+32].cuda()\n",
        "\n",
        "        outputs = classifier(inputs)\n",
        "        _, predicted = outputs.max(1)\n",
        "\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "print('Accuracy on the test set using original model: %d %%' % (100 * correct / total))\n",
        "\n",
        "\n",
        "classifier.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i in range(0, len(remaining_features), 32):\n",
        "        inputs = remaining_features[i:i+32].cuda()\n",
        "        labels = remaining_labels[i:i+32].cuda()\n",
        "\n",
        "        outputs = classifier(inputs)\n",
        "        _, predicted = outputs.max(1)\n",
        "\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "print('Accuracy on the remaining train set using original model: %d %%' % (100 * correct / total))\n",
        "\n",
        "\n",
        "classifier.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i in range(0, len(forget_features), 32):\n",
        "        inputs = forget_features[i:i+32].cuda()\n",
        "        labels = forget_labels[i:i+32].cuda()\n",
        "\n",
        "        outputs = classifier(inputs)\n",
        "        _, predicted = outputs.max(1)\n",
        "\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "print('Accuracy on the forget train set using original model: %d %%' % (100 * correct / total))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVQeGIjUwLTd",
        "outputId": "5dde5547-9810-4133-f943-66037e7e5471"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FPR:0.22, FNR:0.16, FP24.00, TN86.00, TP76.00, FN14.00\n",
            "FPR:0.14, FNR:0.11, FP14.00, TN89.00, TP86.00, FN11.00\n",
            "FPR:0.20, FNR:0.11, FP23.00, TN90.00, TP77.00, FN10.00\n",
            "FPR:0.14, FNR:0.08, FP15.00, TN93.00, TP85.00, FN7.00\n",
            "FPR:0.23, FNR:0.05, FP29.00, TN96.00, TP71.00, FN4.00\n",
            "0.849\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import random\n",
        "\n",
        "def cm_score(estimator, X, y):\n",
        "    y_pred = estimator.predict(X)\n",
        "    cnf_matrix = confusion_matrix(y, y_pred)\n",
        "\n",
        "    FP = cnf_matrix[0][1]\n",
        "    FN = cnf_matrix[1][0]\n",
        "    TP = cnf_matrix[0][0]\n",
        "    TN = cnf_matrix[1][1]\n",
        "\n",
        "\n",
        "    # Sensitivity, hit rate, recall, or true positive rate\n",
        "    TPR = TP/(TP+FN)\n",
        "    # Specificity or true negative rate\n",
        "    TNR = TN/(TN+FP)\n",
        "    # Precision or positive predictive value\n",
        "    PPV = TP/(TP+FP)\n",
        "    # Negative predictive value\n",
        "    NPV = TN/(TN+FN)\n",
        "    # Fall out or false positive rate\n",
        "    FPR = FP/(FP+TN)\n",
        "    # False negative rate\n",
        "    FNR = FN/(TP+FN)\n",
        "    # False discovery rate\n",
        "    FDR = FP/(TP+FP)\n",
        "\n",
        "    # Overall accuracy\n",
        "    ACC = (TP+TN)/(TP+FP+FN+TN)\n",
        "    print (f\"FPR:{FPR:.2f}, FNR:{FNR:.2f}, FP{FP:.2f}, TN{TN:.2f}, TP{TP:.2f}, FN{FN:.2f}\")\n",
        "    return ACC\n",
        "\n",
        "def evaluate_attack_model(sample_loss,\n",
        "                          members,\n",
        "                          n_splits = 100,\n",
        "                          random_state = None):\n",
        "  \"\"\"Computes the cross-validation score of a membership inference attack.\n",
        "  Args:\n",
        "    sample_loss : array_like of shape (n,).\n",
        "      objective function evaluated on n samples.\n",
        "    members : array_like of shape (n,),\n",
        "      whether a sample was used for training.\n",
        "    n_splits: int\n",
        "      number of splits to use in the cross-validation.\n",
        "    random_state: int, RandomState instance or None, default=None\n",
        "      random state to use in cross-validation splitting.\n",
        "  Returns:\n",
        "    score : array_like of size (n_splits,)\n",
        "  \"\"\"\n",
        "\n",
        "  unique_members = np.unique(members)\n",
        "  if not np.all(unique_members == np.array([0, 1])):\n",
        "    raise ValueError(\"members should only have 0 and 1s\")\n",
        "\n",
        "  attack_model = LogisticRegression()\n",
        "  cv = StratifiedShuffleSplit(\n",
        "      n_splits=n_splits, random_state=random_state)\n",
        "  return cross_val_score(attack_model, sample_loss, members, cv=cv, scoring=cm_score)\n",
        "\n",
        "\n",
        "def membership_inference_attack(model, test_f, test_l, forget_f, forget_l, seed):\n",
        "\n",
        "  fgt_cls = list(np.unique(forget_l))\n",
        "  indices = [i in fgt_cls for i in test_l]\n",
        "  test_f = test_f[indices]\n",
        "  test_l = test_l[indices]\n",
        "  cr = nn.CrossEntropyLoss(reduction='none')\n",
        "\n",
        "  test_losses = []\n",
        "  forget_losses = []\n",
        "  bs=128\n",
        "  with torch.no_grad():\n",
        "    for i in range(0, len(test_f), bs):\n",
        "        inputs = test_f[i:i+bs].cuda()\n",
        "        labels = test_l[i:i+bs].cuda()\n",
        "        labels_onehot = one_hot_encode(labels.cuda(), 10)\n",
        "        outputs = model(inputs)\n",
        "        loss = cr(outputs, labels_onehot)\n",
        "        # print(loss)\n",
        "        # loss = quadratic_loss(outputs, labels_onehot)\n",
        "        test_losses = test_losses + list(loss.cpu().detach().numpy())\n",
        "        # test_losses.append(loss.cpu().detach().numpy())\n",
        "    for i in range(0, len(forget_f), bs):\n",
        "        inputs = forget_f[i:i+bs].cuda()\n",
        "        labels = forget_l[i:i+bs].cuda()\n",
        "        labels_onehot = one_hot_encode(labels.cuda(), 10)\n",
        "        outputs = model(inputs)\n",
        "        loss = cr(outputs, labels_onehot)\n",
        "        # print(loss)\n",
        "        # loss = quadratic_loss(outputs, labels_onehot)\n",
        "        forget_losses = forget_losses + list(loss.cpu().detach().numpy())\n",
        "        # forget_losses.append(loss.cpu().detach().numpy())\n",
        "\n",
        "  np.random.seed(seed)\n",
        "  random.seed(seed)\n",
        "  if len(forget_losses) > len(test_losses):\n",
        "      forget_losses = list(random.sample(forget_losses, len(test_losses)))\n",
        "  elif len(test_losses) > len(forget_losses):\n",
        "      test_losses = list(random.sample(test_losses, len(forget_losses)))\n",
        "\n",
        "  t_labels = [0]*len(test_losses)\n",
        "  f_labels = [1]*len(forget_losses)\n",
        "  features = np.array(test_losses + forget_losses).reshape(-1,1)\n",
        "  labels = np.array(t_labels + f_labels).reshape(-1)\n",
        "  # features = np.clip(features, -100, 100)\n",
        "  score = evaluate_attack_model(features, labels, n_splits=5, random_state=seed)\n",
        "\n",
        "  return score\n",
        "\n",
        "\n",
        "\n",
        "score = membership_inference_attack(classifier, test_features, test_labels, forget_features, forget_labels, 2023)\n",
        "\n",
        "print(np.mean(score))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A5s_OnxiKo2V"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "model_MI = classifier\n",
        "\n",
        "remaining_activations = extract_features(model_MI, remaining_features)\n",
        "remaining_activations = np.vstack(remaining_activations)\n",
        "\n",
        "test_activations = extract_features(model_MI, test_features)\n",
        "test_activations = np.vstack(test_activations)\n",
        "\n",
        "forget_activations = extract_features(model_MI, forget_features)\n",
        "forget_activations = np.vstack(forget_activations)\n",
        "\n",
        "train_activations = extract_features(model_MI, train_features)\n",
        "train_activations = np.vstack(train_activations)\n",
        "\n",
        "\n",
        "# binary_classifier, binary_train_features_tensor, binary_train_labels_tensor = train_binary_classifier(model_MI, remaining_activations, remaining_labels, test_activations, test_labels)\n",
        "# binary_classifier, binary_train_features_tensor, binary_train_labels_tensor = train_binary_classifier(model_MI, forget_activations, forget_labels, test_activations, test_labels)\n",
        "\n",
        "binary_classifier, binary_train_features_tensor, binary_train_labels_tensor = train_binary_classifier(model_MI, train_activations, train_labels, test_activations, test_labels)\n",
        "\n",
        "from sklearn import svm\n",
        "\n",
        "binary_model = svm.SVC(kernel='linear')\n",
        "\n",
        "binary_model.fit(binary_train_features_tensor.cpu(), binary_train_labels_tensor.cpu())\n",
        "\n",
        "binary_test_features_tensor = torch.tensor(forget_activations, dtype=torch.float32).cuda()\n",
        "binary_test_outputs = binary_classifier(binary_test_features_tensor)\n",
        "# binary_test_outputs = binary_model.predict(binary_test_features_tensor.cpu())\n",
        "binary_predictions = (torch.sigmoid(torch.Tensor(binary_test_outputs)) > 0.5).cpu().numpy().astype(np.int)\n",
        "\n",
        "# Evaluate the results\n",
        "accuracy_binary = np.mean(binary_predictions == 1)  # Assuming D_f is correctly classified as 1\n",
        "# accuracy_binary = np.mean(binary_test_outputs == 1)\n",
        "print(f\"Original attack success on D_f: {accuracy_binary}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nrl8XOsTpvEu"
      },
      "outputs": [],
      "source": [
        "print(binary_train_labels_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8GB6lDYi8B7k",
        "outputId": "356492ff-2a53-4e93-f510-04ba1df09ea9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Batch 1, Loss: 0.05118412524461746\n",
            "Epoch 1, Batch 101, Loss: 0.0001047474579536356\n",
            "Epoch 1, Batch 201, Loss: 0.0386105440557003\n",
            "Epoch 1, Batch 301, Loss: 0.03160007670521736\n",
            "Epoch 1, Batch 401, Loss: 0.027521003037691116\n",
            "Epoch 1, Batch 501, Loss: 0.02831871621310711\n",
            "Epoch 1, Batch 601, Loss: 0.025489067658782005\n",
            "Epoch 1, Batch 701, Loss: 0.02509658969938755\n",
            "Epoch 1, Batch 801, Loss: 0.026556620374321938\n",
            "Epoch 1, Batch 901, Loss: 0.026451706886291504\n",
            "Epoch 1, Batch 1001, Loss: 0.02388654090464115\n",
            "Epoch 1, Batch 1101, Loss: 0.02197069302201271\n",
            "Epoch 1, Batch 1201, Loss: 0.023602019995450974\n",
            "Epoch 1, Batch 1301, Loss: 0.024699201807379723\n",
            "Epoch 1, Batch 1401, Loss: 0.02371923439204693\n",
            "Epoch 1, Batch 1501, Loss: 0.024827508255839348\n",
            "Epoch 2, Batch 1, Loss: 0.0670236349105835\n",
            "Epoch 2, Batch 101, Loss: 0.005145589355379343\n",
            "Epoch 2, Batch 201, Loss: 0.02523031271994114\n",
            "Epoch 2, Batch 301, Loss: 0.025309771299362183\n",
            "Epoch 2, Batch 401, Loss: 0.022235265001654625\n",
            "Epoch 2, Batch 501, Loss: 0.02402636781334877\n",
            "Epoch 2, Batch 601, Loss: 0.023129060864448547\n",
            "Epoch 2, Batch 701, Loss: 0.02217373065650463\n",
            "Epoch 2, Batch 801, Loss: 0.024831926450133324\n",
            "Epoch 2, Batch 901, Loss: 0.024505434557795525\n",
            "Epoch 2, Batch 1001, Loss: 0.022682780399918556\n",
            "Epoch 2, Batch 1101, Loss: 0.019936515018343925\n",
            "Epoch 2, Batch 1201, Loss: 0.022603807970881462\n",
            "Epoch 2, Batch 1301, Loss: 0.023603742942214012\n",
            "Epoch 2, Batch 1401, Loss: 0.02269820123910904\n",
            "Epoch 2, Batch 1501, Loss: 0.023157857358455658\n",
            "Epoch 3, Batch 1, Loss: 0.06676506251096725\n",
            "Epoch 3, Batch 101, Loss: 0.00584432715550065\n",
            "Epoch 3, Batch 201, Loss: 0.0236698929220438\n",
            "Epoch 3, Batch 301, Loss: 0.02471635304391384\n",
            "Epoch 3, Batch 401, Loss: 0.0212837066501379\n",
            "Epoch 3, Batch 501, Loss: 0.023109853267669678\n",
            "Epoch 3, Batch 601, Loss: 0.022759290412068367\n",
            "Epoch 3, Batch 701, Loss: 0.021719325333833694\n",
            "Epoch 3, Batch 801, Loss: 0.024599332362413406\n",
            "Epoch 3, Batch 901, Loss: 0.0240113977342844\n",
            "Epoch 3, Batch 1001, Loss: 0.022418009117245674\n",
            "Epoch 3, Batch 1101, Loss: 0.0192025788128376\n",
            "Epoch 3, Batch 1201, Loss: 0.022282620891928673\n",
            "Epoch 3, Batch 1301, Loss: 0.023207224905490875\n",
            "Epoch 3, Batch 1401, Loss: 0.022357529029250145\n",
            "Epoch 3, Batch 1501, Loss: 0.022377828136086464\n",
            "Epoch 4, Batch 1, Loss: 0.06654013693332672\n",
            "Epoch 4, Batch 101, Loss: 0.006053078453987837\n",
            "Epoch 4, Batch 201, Loss: 0.023089738562703133\n",
            "Epoch 4, Batch 301, Loss: 0.024465113878250122\n",
            "Epoch 4, Batch 401, Loss: 0.02079450897872448\n",
            "Epoch 4, Batch 501, Loss: 0.02264505624771118\n",
            "Epoch 4, Batch 601, Loss: 0.022540908306837082\n",
            "Epoch 4, Batch 701, Loss: 0.021528542041778564\n",
            "Epoch 4, Batch 801, Loss: 0.024475693702697754\n",
            "Epoch 4, Batch 901, Loss: 0.023775914683938026\n",
            "Epoch 4, Batch 1001, Loss: 0.02228548564016819\n",
            "Epoch 4, Batch 1101, Loss: 0.01879402995109558\n",
            "Epoch 4, Batch 1201, Loss: 0.022081201896071434\n",
            "Epoch 4, Batch 1301, Loss: 0.023004312068223953\n",
            "Epoch 4, Batch 1401, Loss: 0.02217799983918667\n",
            "Epoch 4, Batch 1501, Loss: 0.021942051127552986\n",
            "Epoch 5, Batch 1, Loss: 0.06638439744710922\n",
            "Epoch 5, Batch 101, Loss: 0.00613839365541935\n",
            "Epoch 5, Batch 201, Loss: 0.022774895653128624\n",
            "Epoch 5, Batch 301, Loss: 0.024310221895575523\n",
            "Epoch 5, Batch 401, Loss: 0.020472003147006035\n",
            "Epoch 5, Batch 501, Loss: 0.022354857996106148\n",
            "Epoch 5, Batch 601, Loss: 0.022369487211108208\n",
            "Epoch 5, Batch 701, Loss: 0.02140841819345951\n",
            "Epoch 5, Batch 801, Loss: 0.02437431551516056\n",
            "Epoch 5, Batch 901, Loss: 0.02363443374633789\n",
            "Epoch 5, Batch 1001, Loss: 0.022200580686330795\n",
            "Epoch 5, Batch 1101, Loss: 0.018533146008849144\n",
            "Epoch 5, Batch 1201, Loss: 0.021937739104032516\n",
            "Epoch 5, Batch 1301, Loss: 0.022882763296365738\n",
            "Epoch 5, Batch 1401, Loss: 0.022069070488214493\n",
            "Epoch 5, Batch 1501, Loss: 0.021666621789336205\n",
            "Epoch 6, Batch 1, Loss: 0.06626134365797043\n",
            "Epoch 6, Batch 101, Loss: 0.006183002609759569\n",
            "Epoch 6, Batch 201, Loss: 0.022569632157683372\n",
            "Epoch 6, Batch 301, Loss: 0.02420486882328987\n",
            "Epoch 6, Batch 401, Loss: 0.020235497504472733\n",
            "Epoch 6, Batch 501, Loss: 0.022155847400426865\n",
            "Epoch 6, Batch 601, Loss: 0.022229546681046486\n",
            "Epoch 6, Batch 701, Loss: 0.021323978900909424\n",
            "Epoch 6, Batch 801, Loss: 0.0242889653891325\n",
            "Epoch 6, Batch 901, Loss: 0.02353878878057003\n",
            "Epoch 6, Batch 1001, Loss: 0.022141439840197563\n",
            "Epoch 6, Batch 1101, Loss: 0.0183542612940073\n",
            "Epoch 6, Batch 1201, Loss: 0.021832119673490524\n",
            "Epoch 6, Batch 1301, Loss: 0.0228018369525671\n",
            "Epoch 6, Batch 1401, Loss: 0.021998554468154907\n",
            "Epoch 6, Batch 1501, Loss: 0.021474724635481834\n",
            "Epoch 7, Batch 1, Loss: 0.06615791469812393\n",
            "Epoch 7, Batch 101, Loss: 0.0062113068997859955\n",
            "Epoch 7, Batch 201, Loss: 0.022424157708883286\n",
            "Epoch 7, Batch 301, Loss: 0.024130329489707947\n",
            "Epoch 7, Batch 401, Loss: 0.020052878186106682\n",
            "Epoch 7, Batch 501, Loss: 0.022010724991559982\n",
            "Epoch 7, Batch 601, Loss: 0.022115221247076988\n",
            "Epoch 7, Batch 701, Loss: 0.021262846887111664\n",
            "Epoch 7, Batch 801, Loss: 0.024218125268816948\n",
            "Epoch 7, Batch 901, Loss: 0.023469047620892525\n",
            "Epoch 7, Batch 1001, Loss: 0.022098513320088387\n",
            "Epoch 7, Batch 1101, Loss: 0.018225394189357758\n",
            "Epoch 7, Batch 1201, Loss: 0.021752892062067986\n",
            "Epoch 7, Batch 1301, Loss: 0.022743849083781242\n",
            "Epoch 7, Batch 1401, Loss: 0.02195114456117153\n",
            "Epoch 7, Batch 1501, Loss: 0.02133077383041382\n",
            "Epoch 8, Batch 1, Loss: 0.06606907397508621\n",
            "Epoch 8, Batch 101, Loss: 0.006232172250747681\n",
            "Epoch 8, Batch 201, Loss: 0.022316157817840576\n",
            "Epoch 8, Batch 301, Loss: 0.02407613769173622\n",
            "Epoch 8, Batch 401, Loss: 0.019907904788851738\n",
            "Epoch 8, Batch 501, Loss: 0.021899985149502754\n",
            "Epoch 8, Batch 601, Loss: 0.022021876648068428\n",
            "Epoch 8, Batch 701, Loss: 0.021218156442046165\n",
            "Epoch 8, Batch 801, Loss: 0.024159789085388184\n",
            "Epoch 8, Batch 901, Loss: 0.023415375500917435\n",
            "Epoch 8, Batch 1001, Loss: 0.022066602483391762\n",
            "Epoch 8, Batch 1101, Loss: 0.01812887191772461\n",
            "Epoch 8, Batch 1201, Loss: 0.02169254794716835\n",
            "Epoch 8, Batch 1301, Loss: 0.022700106725096703\n",
            "Epoch 8, Batch 1401, Loss: 0.02191849611699581\n",
            "Epoch 8, Batch 1501, Loss: 0.02121680974960327\n",
            "Epoch 9, Batch 1, Loss: 0.06599191576242447\n",
            "Epoch 9, Batch 101, Loss: 0.006249342113733292\n",
            "Epoch 9, Batch 201, Loss: 0.0222333837300539\n",
            "Epoch 9, Batch 301, Loss: 0.024035805836319923\n",
            "Epoch 9, Batch 401, Loss: 0.019790783524513245\n",
            "Epoch 9, Batch 501, Loss: 0.021812457591295242\n",
            "Epoch 9, Batch 601, Loss: 0.02194545418024063\n",
            "Epoch 9, Batch 701, Loss: 0.02118533104658127\n",
            "Epoch 9, Batch 801, Loss: 0.024111809208989143\n",
            "Epoch 9, Batch 901, Loss: 0.02337242104113102\n",
            "Epoch 9, Batch 1001, Loss: 0.022042592987418175\n",
            "Epoch 9, Batch 1101, Loss: 0.018054192885756493\n",
            "Epoch 9, Batch 1201, Loss: 0.02164597064256668\n",
            "Epoch 9, Batch 1301, Loss: 0.022665897384285927\n",
            "Epoch 9, Batch 1401, Loss: 0.021895674988627434\n",
            "Epoch 9, Batch 1501, Loss: 0.02112305536866188\n",
            "Epoch 10, Batch 1, Loss: 0.06592434644699097\n",
            "Epoch 10, Batch 101, Loss: 0.006264555733650923\n",
            "Epoch 10, Batch 201, Loss: 0.022168336436152458\n",
            "Epoch 10, Batch 301, Loss: 0.024005157873034477\n",
            "Epoch 10, Batch 401, Loss: 0.019694922491908073\n",
            "Epoch 10, Batch 501, Loss: 0.021741347387433052\n",
            "Epoch 10, Batch 601, Loss: 0.021882569417357445\n",
            "Epoch 10, Batch 701, Loss: 0.02116115763783455\n",
            "Epoch 10, Batch 801, Loss: 0.024072250351309776\n",
            "Epoch 10, Batch 901, Loss: 0.023337069898843765\n",
            "Epoch 10, Batch 1001, Loss: 0.0220244862139225\n",
            "Epoch 10, Batch 1101, Loss: 0.01799478754401207\n",
            "Epoch 10, Batch 1201, Loss: 0.021609632298350334\n",
            "Epoch 10, Batch 1301, Loss: 0.022638468071818352\n",
            "Epoch 10, Batch 1401, Loss: 0.021879596635699272\n",
            "Epoch 10, Batch 1501, Loss: 0.021043812856078148\n",
            "Epoch 11, Batch 1, Loss: 0.06586477905511856\n",
            "Epoch 11, Batch 101, Loss: 0.006278665270656347\n",
            "Epoch 11, Batch 201, Loss: 0.022116130217909813\n",
            "Epoch 11, Batch 301, Loss: 0.023981422185897827\n",
            "Epoch 11, Batch 401, Loss: 0.019615614786744118\n",
            "Epoch 11, Batch 501, Loss: 0.02168230153620243\n",
            "Epoch 11, Batch 601, Loss: 0.021830478683114052\n",
            "Epoch 11, Batch 701, Loss: 0.0211433507502079\n",
            "Epoch 11, Batch 801, Loss: 0.024039490148425102\n",
            "Epoch 11, Batch 901, Loss: 0.02330738864839077\n",
            "Epoch 11, Batch 1001, Loss: 0.022010916844010353\n",
            "Epoch 11, Batch 1101, Loss: 0.017946390435099602\n",
            "Epoch 11, Batch 1201, Loss: 0.021581022068858147\n",
            "Epoch 11, Batch 1301, Loss: 0.022616082802414894\n",
            "Epoch 11, Batch 1401, Loss: 0.021868234500288963\n",
            "Epoch 11, Batch 1501, Loss: 0.02097552828490734\n",
            "Epoch 12, Batch 1, Loss: 0.06581201404333115\n",
            "Epoch 12, Batch 101, Loss: 0.00629208842292428\n",
            "Epoch 12, Batch 201, Loss: 0.022073466330766678\n",
            "Epoch 12, Batch 301, Loss: 0.0239627193659544\n",
            "Epoch 12, Batch 401, Loss: 0.019549375399947166\n",
            "Epoch 12, Batch 501, Loss: 0.021632423624396324\n",
            "Epoch 12, Batch 601, Loss: 0.02178700640797615\n",
            "Epoch 12, Batch 701, Loss: 0.021130291745066643\n",
            "Epoch 12, Batch 801, Loss: 0.02401222102344036\n",
            "Epoch 12, Batch 901, Loss: 0.023282108828425407\n",
            "Epoch 12, Batch 1001, Loss: 0.022000927478075027\n",
            "Epoch 12, Batch 1101, Loss: 0.01790614239871502\n",
            "Epoch 12, Batch 1201, Loss: 0.021558335050940514\n",
            "Epoch 12, Batch 1301, Loss: 0.022597601637244225\n",
            "Epoch 12, Batch 1401, Loss: 0.021860232576727867\n",
            "Epoch 12, Batch 1501, Loss: 0.020915880799293518\n",
            "Epoch 13, Batch 1, Loss: 0.06576506793498993\n",
            "Epoch 13, Batch 101, Loss: 0.006305030081421137\n",
            "Epoch 13, Batch 201, Loss: 0.022038046270608902\n",
            "Epoch 13, Batch 301, Loss: 0.02394775114953518\n",
            "Epoch 13, Batch 401, Loss: 0.01949355937540531\n",
            "Epoch 13, Batch 501, Loss: 0.02158970944583416\n",
            "Epoch 13, Batch 601, Loss: 0.021750448271632195\n",
            "Epoch 13, Batch 701, Loss: 0.021120799705386162\n",
            "Epoch 13, Batch 801, Loss: 0.02398940920829773\n",
            "Epoch 13, Batch 901, Loss: 0.023260358721017838\n",
            "Epoch 13, Batch 1001, Loss: 0.02199379913508892\n",
            "Epoch 13, Batch 1101, Loss: 0.01787205971777439\n",
            "Epoch 13, Batch 1201, Loss: 0.021540256217122078\n",
            "Epoch 13, Batch 1301, Loss: 0.022582221776247025\n",
            "Epoch 13, Batch 1401, Loss: 0.021854650229215622\n",
            "Epoch 13, Batch 1501, Loss: 0.020863255485892296\n",
            "Epoch 14, Batch 1, Loss: 0.06572317332029343\n",
            "Epoch 14, Batch 101, Loss: 0.00631757453083992\n",
            "Epoch 14, Batch 201, Loss: 0.022008245810866356\n",
            "Epoch 14, Batch 301, Loss: 0.023935599252581596\n",
            "Epoch 14, Batch 401, Loss: 0.019446149468421936\n",
            "Epoch 14, Batch 501, Loss: 0.02155272662639618\n",
            "Epoch 14, Batch 601, Loss: 0.02171946130692959\n",
            "Epoch 14, Batch 701, Loss: 0.021114036440849304\n",
            "Epoch 14, Batch 801, Loss: 0.0239702221006155\n",
            "Epoch 14, Batch 901, Loss: 0.023241490125656128\n",
            "Epoch 14, Batch 1001, Loss: 0.021988993510603905\n",
            "Epoch 14, Batch 1101, Loss: 0.01784275472164154\n",
            "Epoch 14, Batch 1201, Loss: 0.021525800228118896\n",
            "Epoch 14, Batch 1301, Loss: 0.022569358348846436\n",
            "Epoch 14, Batch 1401, Loss: 0.0218508243560791\n",
            "Epoch 14, Batch 1501, Loss: 0.020816480740904808\n",
            "Epoch 15, Batch 1, Loss: 0.0656856819987297\n",
            "Epoch 15, Batch 101, Loss: 0.006329757161438465\n",
            "Epoch 15, Batch 201, Loss: 0.021982884034514427\n",
            "Epoch 15, Batch 301, Loss: 0.02392559126019478\n",
            "Epoch 15, Batch 401, Loss: 0.01940556801855564\n",
            "Epoch 15, Batch 501, Loss: 0.021520426496863365\n",
            "Epoch 15, Batch 601, Loss: 0.021692993119359016\n",
            "Epoch 15, Batch 701, Loss: 0.021109377965331078\n",
            "Epoch 15, Batch 801, Loss: 0.023954013362526894\n",
            "Epoch 15, Batch 901, Loss: 0.023225029930472374\n",
            "Epoch 15, Batch 1001, Loss: 0.02198607847094536\n",
            "Epoch 15, Batch 1101, Loss: 0.017817219719290733\n",
            "Epoch 15, Batch 1201, Loss: 0.02151421830058098\n",
            "Epoch 15, Batch 1301, Loss: 0.02255856804549694\n",
            "Epoch 15, Batch 1401, Loss: 0.0218482818454504\n",
            "Epoch 15, Batch 1501, Loss: 0.02077466994524002\n",
            "Epoch 16, Batch 1, Loss: 0.06565205007791519\n",
            "Epoch 16, Batch 101, Loss: 0.006341574247926474\n",
            "Epoch 16, Batch 201, Loss: 0.021961087360978127\n",
            "Epoch 16, Batch 301, Loss: 0.02391725592315197\n",
            "Epoch 16, Batch 401, Loss: 0.019370589405298233\n",
            "Epoch 16, Batch 501, Loss: 0.021492013707756996\n",
            "Epoch 16, Batch 601, Loss: 0.021670212969183922\n",
            "Epoch 16, Batch 701, Loss: 0.021106364205479622\n",
            "Epoch 16, Batch 801, Loss: 0.023940254002809525\n",
            "Epoch 16, Batch 901, Loss: 0.023210594430565834\n",
            "Epoch 16, Batch 1001, Loss: 0.02198471687734127\n",
            "Epoch 16, Batch 1101, Loss: 0.017794711515307426\n",
            "Epoch 16, Batch 1201, Loss: 0.021504947915673256\n",
            "Epoch 16, Batch 1301, Loss: 0.02254948765039444\n",
            "Epoch 16, Batch 1401, Loss: 0.02184668369591236\n",
            "Epoch 16, Batch 1501, Loss: 0.02073713019490242\n",
            "Epoch 17, Batch 1, Loss: 0.06562181562185287\n",
            "Epoch 17, Batch 101, Loss: 0.006353020668029785\n",
            "Epoch 17, Batch 201, Loss: 0.021942202001810074\n",
            "Epoch 17, Batch 301, Loss: 0.023910224437713623\n",
            "Epoch 17, Batch 401, Loss: 0.019340237602591515\n",
            "Epoch 17, Batch 501, Loss: 0.021466879174113274\n",
            "Epoch 17, Batch 601, Loss: 0.021650463342666626\n",
            "Epoch 17, Batch 701, Loss: 0.021104643121361732\n",
            "Epoch 17, Batch 801, Loss: 0.023928524926304817\n",
            "Epoch 17, Batch 901, Loss: 0.023197883740067482\n",
            "Epoch 17, Batch 1001, Loss: 0.021984636783599854\n",
            "Epoch 17, Batch 1101, Loss: 0.0177746769040823\n",
            "Epoch 17, Batch 1201, Loss: 0.02149754948914051\n",
            "Epoch 17, Batch 1301, Loss: 0.022541852667927742\n",
            "Epoch 17, Batch 1401, Loss: 0.021845774725079536\n",
            "Epoch 17, Batch 1501, Loss: 0.020703306421637535\n",
            "Epoch 18, Batch 1, Loss: 0.06559456884860992\n",
            "Epoch 18, Batch 101, Loss: 0.006364081986248493\n",
            "Epoch 18, Batch 201, Loss: 0.02192571572959423\n",
            "Epoch 18, Batch 301, Loss: 0.023904232308268547\n",
            "Epoch 18, Batch 401, Loss: 0.019313737750053406\n",
            "Epoch 18, Batch 501, Loss: 0.0214445348829031\n",
            "Epoch 18, Batch 601, Loss: 0.021633224561810493\n",
            "Epoch 18, Batch 701, Loss: 0.021103940904140472\n",
            "Epoch 18, Batch 801, Loss: 0.02391848899424076\n",
            "Epoch 18, Batch 901, Loss: 0.02318665198981762\n",
            "Epoch 18, Batch 1001, Loss: 0.021985603496432304\n",
            "Epoch 18, Batch 1101, Loss: 0.017756687477231026\n",
            "Epoch 18, Batch 1201, Loss: 0.021491674706339836\n",
            "Epoch 18, Batch 1301, Loss: 0.022535432130098343\n",
            "Epoch 18, Batch 1401, Loss: 0.0218453761190176\n",
            "Epoch 18, Batch 1501, Loss: 0.020672734826803207\n",
            "Epoch 19, Batch 1, Loss: 0.06556998938322067\n",
            "Epoch 19, Batch 101, Loss: 0.006374749820679426\n",
            "Epoch 19, Batch 201, Loss: 0.02191123738884926\n",
            "Epoch 19, Batch 301, Loss: 0.02389906719326973\n",
            "Epoch 19, Batch 401, Loss: 0.019290467724204063\n",
            "Epoch 19, Batch 501, Loss: 0.021424589678645134\n",
            "Epoch 19, Batch 601, Loss: 0.021618066355586052\n",
            "Epoch 19, Batch 701, Loss: 0.021104056388139725\n",
            "Epoch 19, Batch 801, Loss: 0.02390987053513527\n",
            "Epoch 19, Batch 901, Loss: 0.023176690563559532\n",
            "Epoch 19, Batch 1001, Loss: 0.02198743261396885\n",
            "Epoch 19, Batch 1101, Loss: 0.01774042285978794\n",
            "Epoch 19, Batch 1201, Loss: 0.021487053483724594\n",
            "Epoch 19, Batch 1301, Loss: 0.022530032321810722\n",
            "Epoch 19, Batch 1401, Loss: 0.021845350041985512\n",
            "Epoch 19, Batch 1501, Loss: 0.020645031705498695\n",
            "Epoch 20, Batch 1, Loss: 0.06554774194955826\n",
            "Epoch 20, Batch 101, Loss: 0.006385012064129114\n",
            "Epoch 20, Batch 201, Loss: 0.021898454055190086\n",
            "Epoch 20, Batch 301, Loss: 0.023894572630524635\n",
            "Epoch 20, Batch 401, Loss: 0.019269926473498344\n",
            "Epoch 20, Batch 501, Loss: 0.021406734362244606\n",
            "Epoch 20, Batch 601, Loss: 0.021604666486382484\n",
            "Epoch 20, Batch 701, Loss: 0.021104812622070312\n",
            "Epoch 20, Batch 801, Loss: 0.023902446031570435\n",
            "Epoch 20, Batch 901, Loss: 0.02316783182322979\n",
            "Epoch 20, Batch 1001, Loss: 0.021989963948726654\n",
            "Epoch 20, Batch 1101, Loss: 0.017725622281432152\n",
            "Epoch 20, Batch 1201, Loss: 0.021483460441231728\n",
            "Epoch 20, Batch 1301, Loss: 0.022525504231452942\n",
            "Epoch 20, Batch 1401, Loss: 0.02184559963643551\n",
            "Epoch 20, Batch 1501, Loss: 0.020619874820113182\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "for epoch in range(20):\n",
        "    for i in range(0, len(remaining_features), 32):\n",
        "        inputs = remaining_features[i:i+32].cuda()\n",
        "        r_labels = remaining_labels[i:i+32].cuda()\n",
        "        # labels_onehot = torch.zeros(r_labels.size(0), num_classes).cuda().scatter_(1, r_labels.view(-1, 1), 1)\n",
        "        labels_onehot_r = one_hot_encode(r_labels.cuda(), num_classes)\n",
        "\n",
        "        outputs = classifier_r(inputs)\n",
        "        # loss = quadratic_loss(outputs, labels_onehot_r, classifier, mu) + psi* additional_regularization(b, classifier.fc.weight)\n",
        "        loss = quadratic_loss(outputs, labels_onehot_r)\n",
        "\n",
        "        optimizer_r.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer_r.step()\n",
        "\n",
        "        if i % 3200 == 0:  # Print training loss every 100 batches\n",
        "            print(f\"Epoch {epoch + 1}, Batch {i // 32 + 1}, Loss: {loss.item()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vntLDsDl_I_a",
        "outputId": "0596e19f-8b67-419c-b744-1f5b3b3122f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy on the test set using remaining set retrain model: 71 %\n",
            "Accuracy on the remaining train set using remaining set retrain model: 72 %\n",
            "Accuracy on the forget train set using remaining set retrain model: 0 %\n"
          ]
        }
      ],
      "source": [
        "classifier_r.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i in range(0, len(test_features), 32):\n",
        "        inputs = test_features[i:i+32].cuda()\n",
        "        labels = test_labels[i:i+32].cuda()\n",
        "\n",
        "        outputs = classifier_r(inputs)\n",
        "        _, predicted = outputs.max(1)\n",
        "\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "print('Accuracy on the test set using remaining set retrain model: %d %%' % (100 * correct / total))\n",
        "\n",
        "\n",
        "classifier_r.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i in range(0, len(remaining_features), 32):\n",
        "        inputs = remaining_features[i:i+32].cuda()\n",
        "        labels = remaining_labels[i:i+32].cuda()\n",
        "\n",
        "        outputs = classifier_r(inputs)\n",
        "        _, predicted = outputs.max(1)\n",
        "\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "print('Accuracy on the remaining train set using remaining set retrain model: %d %%' % (100 * correct / total))\n",
        "\n",
        "\n",
        "classifier_r.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i in range(0, len(forget_features), 32):\n",
        "        inputs = forget_features[i:i+32].cuda()\n",
        "        labels = forget_labels[i:i+32].cuda()\n",
        "\n",
        "        outputs = classifier_r(inputs)\n",
        "        _, predicted = outputs.max(1)\n",
        "\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "print('Accuracy on the forget train set using remaining set retrain model: %d %%' % (100 * correct / total))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VaNsFGwfFc3P",
        "outputId": "8f710929-fceb-453a-d7d4-058afbd83cfc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FPR:0.00, FNR:0.00, FP0.00, TN100.00, TP100.00, FN0.00\n",
            "FPR:0.00, FNR:0.00, FP0.00, TN100.00, TP100.00, FN0.00\n",
            "FPR:0.00, FNR:0.00, FP0.00, TN100.00, TP100.00, FN0.00\n",
            "FPR:0.00, FNR:0.00, FP0.00, TN100.00, TP100.00, FN0.00\n",
            "FPR:0.00, FNR:0.00, FP0.00, TN100.00, TP100.00, FN0.00\n",
            "1.0\n"
          ]
        }
      ],
      "source": [
        "score = membership_inference_attack(classifier_r, test_features, test_labels, forget_features, forget_labels, 2023)\n",
        "\n",
        "print(np.mean(score))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6wECPQdcLIwg"
      },
      "outputs": [],
      "source": [
        "model_MI = classifier_r\n",
        "\n",
        "# remaining_activations = extract_features(model_MI, remaining_features)\n",
        "# remaining_activations = np.vstack(remaining_activations)\n",
        "\n",
        "# test_activations = extract_features(model_MI, test_features)\n",
        "# test_activations = np.vstack(test_activations)\n",
        "\n",
        "forget_activations = extract_features(model_MI, forget_features)\n",
        "forget_activations = np.vstack(forget_activations)\n",
        "\n",
        "\n",
        "# binary_classifier = train_binary_classifier(model_MI, remaining_activations, remaining_labels, test_activations, test_labels)\n",
        "\n",
        "binary_test_features_tensor = torch.tensor(forget_activations, dtype=torch.float32).cuda()\n",
        "binary_test_outputs = binary_classifier(binary_test_features_tensor)\n",
        "# binary_test_outputs = binary_model.predict(binary_test_features_tensor.cpu())\n",
        "binary_predictions = (torch.sigmoid(torch.Tensor(binary_test_outputs)) > 0.5).cpu().numpy().astype(np.int)\n",
        "\n",
        "# Evaluate the results\n",
        "accuracy_binary = np.mean(binary_predictions == 1)  # Assuming D_f is correctly classified as 1\n",
        "# accuracy_binary = np.mean(binary_test_outputs == 1)\n",
        "print(f\"retrain attack success on D_f: {accuracy_binary}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X3qTgbmZ_qPm",
        "outputId": "d72048b5-3618-45d9-8bef-e873da00d7bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fc.weight\n",
            "Parameter fc.weight, shape torch.Size([10, 512])\n",
            "torch.Size([10, 512])\n",
            "torch.Size([10, 512])\n",
            "<class 'torch.Tensor'>\n"
          ]
        }
      ],
      "source": [
        "for name, param in classifier.named_parameters():\n",
        "  print(name)\n",
        "  if name == 'fc.weight':\n",
        "    W = param.data\n",
        "  # else:\n",
        "  #   b = param\n",
        "  print(f'Parameter {name}, shape {param.shape}')\n",
        "  print(param.data.size())\n",
        "\n",
        "# W = param.data.T\n",
        "print(W.shape)\n",
        "print(type(W))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mIalQOyBApAa",
        "outputId": "42cbec4b-94e0-4cd6-a6e9-8bb82ed49b16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.0018, 0.0012, 0.0011,  ..., 0.0012, 0.0014, 0.0011],\n",
            "        [0.0012, 0.0019, 0.0013,  ..., 0.0013, 0.0015, 0.0012],\n",
            "        [0.0011, 0.0013, 0.0018,  ..., 0.0012, 0.0013, 0.0011],\n",
            "        ...,\n",
            "        [0.0012, 0.0013, 0.0012,  ..., 0.0020, 0.0014, 0.0012],\n",
            "        [0.0014, 0.0015, 0.0013,  ..., 0.0014, 0.0025, 0.0014],\n",
            "        [0.0011, 0.0012, 0.0011,  ..., 0.0012, 0.0014, 0.0019]])\n",
            "tensor([[3.5598e-04, 1.4816e-04, 1.5738e-04,  ..., 1.1561e-04, 2.0425e-04,\n",
            "         1.3400e-04],\n",
            "        [1.4816e-04, 1.5064e-04, 1.1564e-04,  ..., 7.1538e-05, 1.2019e-04,\n",
            "         8.6385e-05],\n",
            "        [1.5738e-04, 1.1564e-04, 1.6808e-04,  ..., 7.9275e-05, 1.2939e-04,\n",
            "         9.1316e-05],\n",
            "        ...,\n",
            "        [1.1561e-04, 7.1538e-05, 7.9275e-05,  ..., 9.0230e-05, 8.7953e-05,\n",
            "         7.0204e-05],\n",
            "        [2.0425e-04, 1.2019e-04, 1.2939e-04,  ..., 8.7953e-05, 2.0897e-04,\n",
            "         1.0375e-04],\n",
            "        [1.3400e-04, 8.6385e-05, 9.1316e-05,  ..., 7.0204e-05, 1.0375e-04,\n",
            "         1.2707e-04]])\n",
            "tensor([[0.0018, 0.0012, 0.0011,  ..., 0.0012, 0.0014, 0.0011],\n",
            "        [0.0012, 0.0019, 0.0013,  ..., 0.0013, 0.0015, 0.0012],\n",
            "        [0.0011, 0.0013, 0.0018,  ..., 0.0012, 0.0013, 0.0011],\n",
            "        ...,\n",
            "        [0.0012, 0.0013, 0.0012,  ..., 0.0020, 0.0014, 0.0012],\n",
            "        [0.0014, 0.0015, 0.0013,  ..., 0.0014, 0.0025, 0.0014],\n",
            "        [0.0011, 0.0012, 0.0011,  ..., 0.0012, 0.0014, 0.0019]])\n"
          ]
        }
      ],
      "source": [
        "H_r = remaining_features.T@remaining_features/len(train_features)\n",
        "# H_r = remaining_features.T@remaining_features/len(remaining_features)\n",
        "H_f = forget_features.T@forget_features/len(train_features)\n",
        "# H_f = forget_features.T@forget_features/len(forget_features)\n",
        "H = train_features.T@train_features/len(train_features)\n",
        "print(H_r)\n",
        "print(H_f)\n",
        "print(H)\n",
        "# print(H_f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H66EASqNK0QZ",
        "outputId": "5c91f64d-f12c-442c-8223-26afaf479529"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(5)\n",
            "tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], device='cuda:0')\n",
            "<class 'torch.Tensor'>\n",
            "<class 'torch.Tensor'>\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([512, 10])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-13-e9f2b029aa73>:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  forget_features = torch.tensor(forget_features).cuda()\n"
          ]
        }
      ],
      "source": [
        "# forget_labels_onehot = torch.zeros(forget_labels.size(0), num_classes).cuda().scatter_(1, labels.view(-1, 1), 1)\n",
        "forget_labels_onehot = one_hot_encode(forget_labels.cuda(), num_classes)\n",
        "\n",
        "\n",
        "# if len(forget_labels.shape) == 1:\n",
        "#    forget_labels = forget_labels.unsqueeze(1)\n",
        "\n",
        "# forget_labels_onehot = torch.zeros(forget_labels.size(0), num_classes)\n",
        "# forget_labels_onehot.cuda().scatter_(1, forget_labels.cuda(), 1)\n",
        "\n",
        "print(forget_labels[16])\n",
        "print(forget_labels_onehot[16])\n",
        "\n",
        "forget_features = torch.tensor(forget_features).cuda()\n",
        "print(type(forget_features))\n",
        "print(type(forget_labels_onehot))\n",
        "print(type(W))\n",
        "grad_f = (forget_features.T@(forget_features@W.T-forget_labels_onehot.cuda())/len(train_features))\n",
        "# grad_f = (forget_features.T@(forget_features@W.T-forget_labels_onehot.cuda())/len(forget_features))\n",
        "print(grad_f.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAjy7zxCNN6E",
        "outputId": "2307fbcc-3aaf-429f-f63a-e00673a58d9f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-14-03f43cf63778>:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  grad_f = torch.tensor(grad_f)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "grad_f = torch.tensor(grad_f)\n",
        "hess_grad = torch.linalg.inv(H_r).cuda()@grad_f\n",
        "\n",
        "# hess_grad = len(remaining_features)*torch.linalg.inv(H).cuda()@grad_f/len(train_features)\n",
        "# hess_grad = torch.linalg.inv(H).cuda()@grad_f\n",
        "W_new = W.T+hess_grad.cuda()\n",
        "# print(W_new)\n",
        "pretrained_dict = classifier.state_dict()\n",
        "pretrained_dict['fc.weight'] = W_new.T\n",
        "classifier.load_state_dict(pretrained_dict)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5kQ-QVa1OZVB",
        "outputId": "cb5d9fc7-224d-4c71-9b36-4b270dfc9655"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy on the test set using actual scrubbed model: 75 %\n",
            "Accuracy on the remaining train set using actual scrubbed model: 76 %\n",
            "Accuracy on the forget train set using actual scrubbed model: 47 %\n"
          ]
        }
      ],
      "source": [
        "classifier.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i in range(0, len(test_features), 32):\n",
        "        inputs = test_features[i:i+32].cuda()\n",
        "        labels = test_labels[i:i+32].cuda()\n",
        "\n",
        "        outputs = classifier(inputs)\n",
        "        _, predicted = outputs.max(1)\n",
        "\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "print('Accuracy on the test set using actual scrubbed model: %d %%' % (100 * correct / total))\n",
        "\n",
        "\n",
        "classifier.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i in range(0, len(remaining_features), 32):\n",
        "        inputs = remaining_features[i:i+32].cuda()\n",
        "        labels = remaining_labels[i:i+32].cuda()\n",
        "\n",
        "        outputs = classifier(inputs)\n",
        "        _, predicted = outputs.max(1)\n",
        "\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "print('Accuracy on the remaining train set using actual scrubbed model: %d %%' % (100 * correct / total))\n",
        "\n",
        "\n",
        "classifier.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i in range(0, len(forget_features), 32):\n",
        "        inputs = forget_features[i:i+32].cuda()\n",
        "        labels = forget_labels[i:i+32].cuda()\n",
        "\n",
        "        outputs = classifier(inputs)\n",
        "        _, predicted = outputs.max(1)\n",
        "\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "print('Accuracy on the forget train set using actual scrubbed model: %d %%' % (100 * correct / total))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_2M_GQGFvSP",
        "outputId": "3cce9474-7a2a-4886-eddd-5258b1d754c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FPR:0.38, FNR:0.00, FP61.00, TN100.00, TP39.00, FN0.00\n",
            "FPR:0.39, FNR:0.00, FP63.00, TN100.00, TP37.00, FN0.00\n",
            "FPR:0.37, FNR:0.00, FP59.00, TN100.00, TP41.00, FN0.00\n",
            "FPR:0.38, FNR:0.00, FP60.00, TN100.00, TP40.00, FN0.00\n",
            "FPR:0.30, FNR:0.00, FP43.00, TN100.00, TP57.00, FN0.00\n",
            "0.7140000000000001\n"
          ]
        }
      ],
      "source": [
        "score = membership_inference_attack(classifier, test_features, test_labels, forget_features, forget_labels, 2023)\n",
        "\n",
        "print(np.mean(score))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "id": "p2ZXKvfhLNl4",
        "outputId": "6788893a-407d-4d0b-e95b-ec4f0df1f426"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'binary_classifier' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-c70aab0186be>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mbinary_test_features_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforget_activations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mbinary_test_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbinary_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbinary_test_features_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;31m# binary_test_outputs = binary_model.predict(binary_test_features_tensor.cpu())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mbinary_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbinary_test_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'binary_classifier' is not defined"
          ]
        }
      ],
      "source": [
        "model_MI = classifier\n",
        "\n",
        "# remaining_activations = extract_features(model_MI, remaining_features)\n",
        "# remaining_activations = np.vstack(remaining_activations)\n",
        "\n",
        "# test_activations = extract_features(model_MI, test_features)\n",
        "# test_activations = np.vstack(test_activations)\n",
        "\n",
        "forget_activations = extract_features(model_MI, forget_features)\n",
        "forget_activations = np.vstack(forget_activations)\n",
        "\n",
        "\n",
        "# binary_classifier = train_binary_classifier(model_MI, remaining_activations, remaining_labels, test_activations, test_labels)\n",
        "\n",
        "binary_test_features_tensor = torch.tensor(forget_activations, dtype=torch.float32).cuda()\n",
        "binary_test_outputs = binary_classifier(binary_test_features_tensor)\n",
        "# binary_test_outputs = binary_model.predict(binary_test_features_tensor.cpu())\n",
        "binary_predictions = (torch.sigmoid(torch.Tensor(binary_test_outputs)) > 0.5).cpu().numpy().astype(np.int)\n",
        "\n",
        "# Evaluate the results\n",
        "accuracy_binary = np.mean(binary_predictions == 1)  # Assuming D_f is correctly classified as 1\n",
        "# accuracy_binary = np.mean(binary_test_outputs == 1)\n",
        "print(f\"Original scrubbed attack success on D_f: {accuracy_binary}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9B1jjEI3PHQW",
        "outputId": "8cd5cb38-5d45-4ca4-93a1-d8ec06f5b658"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([50000])\n",
            "tensor([[0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000],\n",
            "        [0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000],\n",
            "        [0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000],\n",
            "        [0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000],\n",
            "        [0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000],\n",
            "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000],\n",
            "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000,\n",
            "         0.0000],\n",
            "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000,\n",
            "         0.0000],\n",
            "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000,\n",
            "         0.0000],\n",
            "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.1000]], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "# if len(train_labels.shape) == 1:\n",
        "#     train_labels = train_labels.unsqueeze(1)\n",
        "\n",
        "# train_labels_onehot = torch.zeros(train_labels.size(0), num_classes)\n",
        "# train_labels_onehot.scatter_(1, train_labels, 1)\n",
        "train_labels_onehot = one_hot_encode(train_labels.cuda(), num_classes)\n",
        "# print(one_hot.shape)  # torch.Size([50000, 10])\n",
        "# train_labels_onehot = torch.zeros(train_labels.size(0), 10).cuda().scatter_(1, labels.view(-1, 1), 1)\n",
        "print(train_labels.shape)\n",
        "Y_train = train_labels_onehot.T@train_labels_onehot/len(train_features)\n",
        "Y = Y_train.cpu().numpy()\n",
        "print(Y_train)\n",
        "W = W.cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Z-UHHRcQKKi"
      },
      "outputs": [],
      "source": [
        "# import cvxpy as cp\n",
        "# import numpy as np\n",
        "\n",
        "# # Create constant matrices\n",
        "# # C = np.array([[2, -1], [-1, 3]])\n",
        "# # A1 = np.array([[1, 0], [0, 0]])\n",
        "# # A2 = np.array([[0, 0], [0, 1]])\n",
        "# d=512\n",
        "# lambda_max_value = d\n",
        "# # Define variables\n",
        "# X = cp.Variable((d, d), symmetric=True)\n",
        "\n",
        "# # Define constraints\n",
        "# constraints = [X >> 0]\n",
        "# constraints += [X >> H_f.cpu().numpy()]\n",
        "# # constraints += [X - lambda_max_value * torch.eye(d).numpy() << 0]\n",
        "# # constraints +=[cp.trace(X) <= 1]\n",
        "# constraints +=[cp.trace(X) >= 0]\n",
        "# # constraints += [X[i, i] == 1 for i in range(d)]\n",
        "# #constraints += [cp.trace(w@X.T@X@w.T) == len(y_train)]\n",
        "\n",
        "# # Define objective\n",
        "# # objective = cp.Minimize(cp.trace((w@X@w.T-Y_train)))\n",
        "\n",
        "# objective = cp.Minimize(cp.trace((W@X@W.T/len(train_features)-Y)))\n",
        "# # objective = cp.Minimize(cp.trace((Y_train-w@X.T@y_train)))\n",
        "\n",
        "# # objective = cp.Minimize(cp.norm(W@X@W.T/len(train_dataset)-Y, 'fro')**2)\n",
        "\n",
        "# # Define problem\n",
        "# problem = cp.Problem(objective, constraints)\n",
        "\n",
        "# # Solve problem\n",
        "# problem.solve(qcp = True)\n",
        "# # problem.solve()\n",
        "\n",
        "# print(\"Optimal value: \", problem.value)\n",
        "# print(\"Optimal variable X: \")\n",
        "# # print(X.value)\n",
        "# err = X.value-H_f.cpu().numpy()-H_r.cpu().numpy()\n",
        "# # print(X.value)\n",
        "# print(np.sqrt(np.trace(np.dot(err.T,err)))/d)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TgjYHa54GzdT",
        "outputId": "b88e5f73-91af-465c-fa58-16df35a181d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True Hessian:\n",
            "tensor([[0.0018, 0.0012, 0.0011,  ..., 0.0012, 0.0014, 0.0011],\n",
            "        [0.0012, 0.0019, 0.0013,  ..., 0.0013, 0.0015, 0.0012],\n",
            "        [0.0011, 0.0013, 0.0018,  ..., 0.0012, 0.0013, 0.0011],\n",
            "        ...,\n",
            "        [0.0012, 0.0013, 0.0012,  ..., 0.0020, 0.0014, 0.0012],\n",
            "        [0.0014, 0.0015, 0.0013,  ..., 0.0014, 0.0025, 0.0014],\n",
            "        [0.0011, 0.0012, 0.0011,  ..., 0.0012, 0.0014, 0.0019]])\n",
            "\n",
            "Estimated Hessian:\n",
            "[[ 0.34028893  0.02220291  0.02177161 ...  0.01165315  0.01164646\n",
            "   0.01131745]\n",
            " [ 0.02220291  0.33128896  0.02959077 ...  0.01495659 -0.00177099\n",
            "   0.01300776]\n",
            " [ 0.02177161  0.02959077  0.31818157 ...  0.00868902  0.00576731\n",
            "   0.01626491]\n",
            " ...\n",
            " [ 0.01165315  0.01495659  0.00868902 ...  0.31652271 -0.00684098\n",
            "   0.01006389]\n",
            " [ 0.01164646 -0.00177099  0.00576731 ... -0.00684098  0.32839508\n",
            "  -0.00221488]\n",
            " [ 0.01131745  0.01300776  0.01626491 ...  0.01006389 -0.00221488\n",
            "   0.31349571]]\n",
            "\n",
            "Difference (Frobenius norm): 3.240337849929511e-05\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import cvxpy as cp\n",
        "# import numpy as np\n",
        "\n",
        "true_hessian = H\n",
        "dim = 512\n",
        "# Perturbations and observed gradient changes\n",
        "num_perturbations = 500\n",
        "delta_w = torch.randn(num_perturbations, dim) * 0.01  # Small perturbations\n",
        "delta_L = []\n",
        "\n",
        "# optimal_w = next(model.parameters()).detach().clone()\n",
        "for dw in delta_w:\n",
        "    # print(W.shape)\n",
        "    # dw[np.newaxis, :]\n",
        "    dw = np.reshape(dw, (1, 512))\n",
        "    # print(dw.shape)\n",
        "    W_noisy = W + dw.numpy()\n",
        "    # print(W_noisy.shape)\n",
        "    W_noisy = torch.tensor(W_noisy)\n",
        "    # pretrained_dict = model.state_dict()\n",
        "    # pretrained_dict['fc.weight'] = W_noisy\n",
        "    # model.load_state_dict(pretrained_dict)\n",
        "    classifier.fc.weight.data =  W_noisy.cuda()\n",
        "    # outputs = model(X_train)\n",
        "    # loss_perturbed = criterion(outputs, y_train.float())\n",
        "    outputs = classifier(forget_features)\n",
        "    # print(outputs.shape)\n",
        "    # print(forget_labels.shape)\n",
        "    # loss_perturbed = quadratic_loss(outputs, forget_labels_onehot, classifier, mu) + psi* additional_regularization(b, classifier.fc.weight)\n",
        "    loss_perturbed = quadratic_loss(outputs, forget_labels_onehot)\n",
        "    # delta_L.append(loss_perturbed.item() - loss.item())\n",
        "    delta_L.append(loss_perturbed.item() - loss_f.item())\n",
        "\n",
        "delta_L = np.array(delta_L)\n",
        "delta_L = delta_L.reshape(-1, 1)\n",
        "\n",
        "# CVXPY problem to estimate Hessian\n",
        "d = dim\n",
        "X = cp.Variable((d, d), symmetric=True)\n",
        "\n",
        "# Create a list to hold our quadratic forms for each perturbation\n",
        "delta_w_matrix = np.stack([dw.numpy() for dw in delta_w])\n",
        "\n",
        "# Calculate the quadratic forms more efficiently\n",
        "quadratic_forms_vectorized = 0.5 * cp.sum(cp.multiply(delta_w_matrix @ X, delta_w_matrix), axis=1)\n",
        "\n",
        "objective = cp.Minimize(cp.sum_squares(delta_L.flatten() - quadratic_forms_vectorized))\n",
        "\n",
        "# Assuming the same constraints\n",
        "# constraints = [X >> 0, cp.trace(X) <= 1, cp.trace(X) >= 0, X >> H_f]\n",
        "constraints = [X >> 0, cp.trace(X) >= 0, X >> H_f]\n",
        "# constraints += [H[i, i] <= 0.25 for i in range(d)]\n",
        "prob = cp.Problem(objective, constraints)\n",
        "prob.solve()\n",
        "\n",
        "# Estimated Hessian\n",
        "H_value = X.value\n",
        "\n",
        "# Compute difference between true and estimated Hessian\n",
        "hessian_diff = np.linalg.norm(H - H_value, 'fro')\n",
        "\n",
        "print(\"True Hessian:\")\n",
        "print(true_hessian)\n",
        "print(\"\\nEstimated Hessian:\")\n",
        "print(H_value)\n",
        "\n",
        "print(\"\\nDifference (Frobenius norm):\", hessian_diff/d**2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFsWTAr3PJoq",
        "outputId": "2e092262-202f-4bc6-e87b-90165ae0e9ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[ 0.34028893  0.02220291  0.02177161 ...  0.01165315  0.01164646\n",
            "   0.01131745]\n",
            " [ 0.02220291  0.33128896  0.02959077 ...  0.01495659 -0.00177099\n",
            "   0.01300776]\n",
            " [ 0.02177161  0.02959077  0.31818157 ...  0.00868902  0.00576731\n",
            "   0.01626491]\n",
            " ...\n",
            " [ 0.01165315  0.01495659  0.00868902 ...  0.31652271 -0.00684098\n",
            "   0.01006389]\n",
            " [ 0.01164646 -0.00177099  0.00576731 ... -0.00684098  0.32839508\n",
            "  -0.00221488]\n",
            " [ 0.01131745  0.01300776  0.01626491 ...  0.01006389 -0.00221488\n",
            "   0.31349571]]\n"
          ]
        }
      ],
      "source": [
        "print(X.value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBJCtcaFTe4K",
        "outputId": "773efc91-8426-4b72-8ea5-ca1d8bb0c52e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'torch.Tensor'>\n",
            "<class 'torch.Tensor'>\n",
            "<class 'torch.Tensor'>\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from torch.autograd import Variable\n",
        "# H_r_aprx = X.value/len(train_features) - H_f.cpu().numpy()\n",
        "H_r_aprx = X.value - H_f.cpu().numpy()\n",
        "H_r_aprx = Variable(torch.Tensor(H_r_aprx).float()).cuda()\n",
        "H_aprx = Variable(torch.Tensor(X.value).float()).cuda()\n",
        "hess_grad_aprx = torch.linalg.inv(H_r_aprx)@grad_f\n",
        "\n",
        "# hess_grad_aprx = len(remaining_features)*torch.linalg.inv(H_aprx)@grad_f/len(train_features)\n",
        "# hess_grad_aprx = torch.linalg.inv(H_aprx)@grad_f\n",
        "W = Variable(torch.Tensor(W).float()).cuda()\n",
        "print(type(hess_grad_aprx))\n",
        "print(type(W))\n",
        "W_new_aprx = W.T + hess_grad_aprx.cuda()\n",
        "print(type(W_new_aprx))\n",
        "pretrained_dict = classifier.state_dict()\n",
        "pretrained_dict['fc.weight'] = W_new_aprx.T\n",
        "classifier.load_state_dict(pretrained_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qK2-QQIPT6G-",
        "outputId": "5013fae6-f3a7-4f27-aa49-fdb0e8ae6082"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy on the test set using approximate scrubbed model: 75 %\n",
            "Accuracy on the remaining train set using approximate scrubbed model: 76 %\n",
            "Accuracy on the forget train set using approximate scrubbed model: 46 %\n"
          ]
        }
      ],
      "source": [
        "classifier.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i in range(0, len(test_features), 32):\n",
        "        inputs = test_features[i:i+32].cuda()\n",
        "        labels = test_labels[i:i+32].cuda()\n",
        "\n",
        "        outputs = classifier(inputs)\n",
        "        _, predicted = outputs.max(1)\n",
        "\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "print('Accuracy on the test set using approximate scrubbed model: %d %%' % (100 * correct / total))\n",
        "\n",
        "\n",
        "classifier.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i in range(0, len(remaining_features), 32):\n",
        "        inputs = remaining_features[i:i+32].cuda()\n",
        "        labels = remaining_labels[i:i+32].cuda()\n",
        "\n",
        "        outputs = classifier(inputs)\n",
        "        _, predicted = outputs.max(1)\n",
        "\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "print('Accuracy on the remaining train set using approximate scrubbed model: %d %%' % (100 * correct / total))\n",
        "\n",
        "\n",
        "classifier.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i in range(0, len(forget_features), 32):\n",
        "        inputs = forget_features[i:i+32].cuda()\n",
        "        labels = forget_labels[i:i+32].cuda()\n",
        "\n",
        "        outputs = classifier(inputs)\n",
        "        _, predicted = outputs.max(1)\n",
        "\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "print('Accuracy on the forget train set using approximate scrubbed model: %d %%' % (100 * correct / total))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lmnrmm-eIeWe",
        "outputId": "88703a5c-91fb-4da9-b13a-056dd8194985"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FPR:0.37, FNR:0.00, FP58.00, TN100.00, TP42.00, FN0.00\n",
            "FPR:0.38, FNR:0.00, FP60.00, TN100.00, TP40.00, FN0.00\n",
            "FPR:0.36, FNR:0.00, FP57.00, TN100.00, TP43.00, FN0.00\n",
            "FPR:0.37, FNR:0.00, FP58.00, TN100.00, TP42.00, FN0.00\n",
            "FPR:0.30, FNR:0.00, FP42.00, TN100.00, TP58.00, FN0.00\n",
            "0.725\n"
          ]
        }
      ],
      "source": [
        "score = membership_inference_attack(classifier, test_features, test_labels, forget_features, forget_labels, 2023)\n",
        "\n",
        "print(np.mean(score))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "id": "73yvKpQILVa7",
        "outputId": "f3564ebf-687e-4464-fde7-13c8d99e6d0c"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'binary_classifier' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-a0b367c7a9c3>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mbinary_test_features_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforget_activations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mbinary_test_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbinary_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbinary_test_features_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;31m# binary_test_outputs = binary_model.predict(binary_test_features_tensor.cpu())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mbinary_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbinary_test_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'binary_classifier' is not defined"
          ]
        }
      ],
      "source": [
        "model_MI = classifier\n",
        "\n",
        "# remaining_activations = extract_features(model_MI, remaining_features)\n",
        "# remaining_activations = np.vstack(remaining_activations)\n",
        "\n",
        "# test_activations = extract_features(model_MI, test_features)\n",
        "# test_activations = np.vstack(test_activations)\n",
        "\n",
        "forget_activations = extract_features(model_MI, forget_features)\n",
        "forget_activations = np.vstack(forget_activations)\n",
        "\n",
        "\n",
        "# binary_classifier = train_binary_classifier(model_MI, remaining_activations, remaining_labels, test_activations, test_labels)\n",
        "\n",
        "binary_test_features_tensor = torch.tensor(forget_activations, dtype=torch.float32).cuda()\n",
        "binary_test_outputs = binary_classifier(binary_test_features_tensor)\n",
        "# binary_test_outputs = binary_model.predict(binary_test_features_tensor.cpu())\n",
        "binary_predictions = (torch.sigmoid(torch.tensor(binary_test_outputs)) > 0.5).cpu().numpy().astype(np.int)\n",
        "\n",
        "# Evaluate the results\n",
        "accuracy_binary = np.mean(binary_predictions == 1)  # Assuming D_f is correctly classified as 1\n",
        "# accuracy_binary = np.mean(binary_test_outputs == 1)\n",
        "print(f\"Approx scrubbed attack success on D_f: {accuracy_binary}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
