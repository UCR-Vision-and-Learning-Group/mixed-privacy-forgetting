{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# date: 04.29.24"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "31X412tX5w9Y",
    "outputId": "2034fd57-e435-4cc9-9af5-bcc2c75217da"
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet18,resnet50,vgg16\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load CIFAR-10 dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "num_classes = 10\n",
    "\n",
    "def one_hot_encode(labels, num_classes):\n",
    "    '''\n",
    "    Convert an array of labels to a one-hot encoded matrix.\n",
    "\n",
    "    Args:\n",
    "    - labels (Tensor): A 1D tensor containing the labels.\n",
    "    - num_classes (int): The number of classes.\n",
    "\n",
    "    Returns:\n",
    "    - Tensor: A matrix where each row is the one-hot encoded version of the corresponding label.\n",
    "    '''\n",
    "    # Create an empty tensor filled with zeros\n",
    "    device = labels.device  # Get the device of the labels tensor\n",
    "    one_hot = torch.zeros(labels.size(0), num_classes, device=device)\n",
    "    # one_hot = torch.zeros(labels.size(0), num_classes)\n",
    "\n",
    "    # Fill the locations corresponding to the labels with ones\n",
    "    one_hot.scatter_(1, labels.unsqueeze(1), 1)\n",
    "\n",
    "    return one_hot\n",
    "\n",
    "# 2. Feature extraction using pretrained model\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        model = resnet18(pretrained=True)\n",
    "        self.features = nn.Sequential(*list(model.children())[:-1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return x\n",
    "\n",
    "def l2_regularization(model):\n",
    "    l2_reg = 0.0\n",
    "    for param in model.parameters():\n",
    "        l2_reg += torch.norm(param, p=2)\n",
    "    return l2_reg\n",
    "\n",
    "#  def additional_regularization(b, w):\n",
    "    # return torch.dot(b, w.view(-1))\n",
    "\n",
    "def normalize_columns(tensor):\n",
    "    \"\"\"\n",
    "    Normalize the columns of the input tensor such that the diagonal entries\n",
    "    of tensor.T @ tensor are 1.\n",
    "    \"\"\"\n",
    "    norms = tensor.norm(p=2, dim=0)  # Compute L2-norm for each column\n",
    "    return tensor / norms\n",
    "\n",
    "def bound_norm(data, upper_bound):\n",
    "    norms = data.norm(dim=1, keepdim=True)\n",
    "    scale = upper_bound / norms\n",
    "    scale[scale > 1] = 1  # Only scale vectors with norm > upper_bound\n",
    "    return data * scale\n",
    "\n",
    "feature_extractor = FeatureExtractor().cuda()\n",
    "feat_dim = 512\n",
    "\n",
    "# Extract features for entire training set\n",
    "train_features = []\n",
    "train_labels_list = []\n",
    "for inputs, labels in trainloader:\n",
    "    inputs = inputs.cuda()\n",
    "    features = feature_extractor(inputs).detach().cpu()\n",
    "    train_features.append(features)\n",
    "    train_labels_list.append(labels)\n",
    "\n",
    "train_features = torch.cat(train_features, dim=0)\n",
    "train_labels = torch.cat(train_labels_list, dim=0)\n",
    "\n",
    "print(train_features.shape)\n",
    "\n",
    "# train_features =  normalize_columns(train_features)\n",
    "train_features =  bound_norm(train_features,1)\n",
    "\n",
    "# train_feature_mean = train_features.mean(dim=0, keepdim=True)\n",
    "# train_feature_variance = train_features.var(dim=0, keepdim=True)\n",
    "\n",
    "# print(train_feature_mean.shape)\n",
    "# # print(train_feature_variance.shape)\n",
    "\n",
    "# train_features = (train_features-train_feature_mean)/train_feature_variance\n",
    "\n",
    "# 3. Linear classifier without bias\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "classifier = Classifier(feat_dim, num_classes).cuda()\n",
    "\n",
    "# 4. Quadratic loss\n",
    "# def quadratic_loss(outputs, targets, classifier, mu):\n",
    "#     return torch.mean(0.5*(outputs - targets)**2 + 0.5*mu*torch.norm((classifier.fc.weight))**2)\n",
    "\n",
    "def quadratic_loss(outputs, targets):\n",
    "    return torch.mean(0.5*(outputs - targets)**2)\n",
    "\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "classifier_r = Classifier(feat_dim, num_classes).cuda()\n",
    "optimizer_r = optim.Adam(classifier_r.parameters(), lr=0.001)\n",
    "\n",
    "mu = 0.000001\n",
    "psi = 0.01\n",
    "\n",
    "# b = torch.from_numpy(np.random.exponential(size=512)).float().cuda()\n",
    "\n",
    "# 5. Train the classifier using the pre-extracted features\n",
    "for epoch in range(10):\n",
    "    for i in range(0, len(train_features), 32):\n",
    "        inputs = train_features[i:i+32].cuda()\n",
    "        labels = train_labels[i:i+32].cuda()\n",
    "        # labels_onehot = torch.zeros(labels.size(0), num_classes).cuda().scatter_(1, labels.view(-1, 1), 1)\n",
    "        labels_onehot = one_hot_encode(labels.cuda(), num_classes)\n",
    "\n",
    "        outputs = classifier(inputs)\n",
    "        # loss = quadratic_loss(outputs, labels_onehot, classifier, mu) + psi* additional_regularization(b, classifier.fc.weight)\n",
    "        loss = quadratic_loss(outputs, labels_onehot)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 3200 == 0:  # Print training loss every 100 batches\n",
    "            print(f\"Epoch {epoch + 1}, Batch {i // 32 + 1}, Loss: {loss.item()}\")\n",
    "\n",
    "# 6. Evaluate the classifier on the test set using pre-extracted features\n",
    "classifier.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "test_features = []\n",
    "test_labels_list = []\n",
    "for inputs, labels in testloader:\n",
    "    inputs = inputs.cuda()\n",
    "    features = feature_extractor(inputs).detach().cpu()\n",
    "    test_features.append(features)\n",
    "    test_labels_list.append(labels)\n",
    "\n",
    "test_features = torch.cat(test_features, dim=0)\n",
    "test_labels = torch.cat(test_labels_list, dim=0)\n",
    "\n",
    "# test_features = (test_features-train_feature_mean)/train_feature_variance\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wLGZWy019N5F",
    "outputId": "7a384893-85a2-48ab-bb13-b2a1ec3df86d"
   },
   "source": [
    "print(train_features.size())\n",
    "print(type(train_features))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IcmqXe6z6mOx",
    "outputId": "eb4739e2-9380-4429-c277-4c04500788eb"
   },
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "# from torch.utils.data import Dataset, Subset\n",
    "\n",
    "# remain_size = int(0.6 * len(trainset))\n",
    "# forget_size = len(trainset) - remain_size\n",
    "\n",
    "\n",
    "forget_class = 5\n",
    "\n",
    "forget_indices = torch.where(train_labels == forget_class)[0]\n",
    "\n",
    "# # Split the dataset into two parts: class 5 and the remaining classes\n",
    "# remaining_indices = torch.where(train_labels != forget_class)[0]\n",
    "\n",
    "# forget_indices = torch.tensor(forget_indices)\n",
    "# remaining_indices = torch.tensor(remaining_indices)\n",
    "\n",
    "\n",
    "# print(trainset)\n",
    "# print(forget_indices)\n",
    "\n",
    "# remaining_features, forget_features =  random_split(train_features, [remain_size, forget_size])\n",
    "# remaining_indices = remaining_features.indices\n",
    "# remaining_labels = train_labels[remaining_indices]\n",
    "# forget_indices = forget_features.indices\n",
    "# forget_labels = train_labels[forget_indices]\n",
    "# print(type(remaining_features))\n",
    "\n",
    "num_forget_samples = int(0.8 * len(forget_indices))\n",
    "\n",
    "# Randomly select indices for the forget set\n",
    "forget_indices_sub = torch.randperm(len(forget_indices))[:num_forget_samples]\n",
    "\n",
    "# Remaining indices for class 5 (10%)\n",
    "remaining_indices_class5 = forget_indices[list(set(range(len(forget_indices))) - set(forget_indices_sub))]\n",
    "# remaining_indices_class5 = list(set(range(len(forget_indices))) - set(forget_indices_sub))\n",
    "\n",
    "forget_indices = forget_indices[forget_indices_sub]\n",
    "\n",
    "\n",
    "\n",
    "# Remaining indices for other classes\n",
    "remaining_indices_other_classes = torch.where(train_labels != forget_class)[0]\n",
    "\n",
    "# Concatenate indices for the remaining set\n",
    "remaining_indices = torch.cat((remaining_indices_class5, remaining_indices_other_classes))\n",
    "\n",
    "\n",
    "forget_features = train_features[forget_indices]\n",
    "remaining_features = train_features[remaining_indices]\n",
    "\n",
    "forget_labels = train_labels[forget_indices]\n",
    "remaining_labels = train_labels[remaining_indices]\n",
    "\n",
    "\n",
    "tensors_rf = [remaining_features[i] for i in range(len(remaining_features))]\n",
    "tensors_rl = [remaining_labels[i] for i in range(len(remaining_labels))]\n",
    "remaining_features = torch.stack(tensors_rf)\n",
    "remaining_labels = torch.stack(tensors_rl)\n",
    "print(type(remaining_labels))\n",
    "print(remaining_labels.size())\n",
    "\n",
    "tensors_ff = [forget_features[i] for i in range(len(forget_features))]\n",
    "tensors_fl = [forget_labels[i] for i in range(len(forget_labels))]\n",
    "forget_features = torch.stack(tensors_ff)\n",
    "forget_labels = torch.stack(tensors_fl)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yGK82303KmZG"
   },
   "source": [
    "def extract_features(model, features):\n",
    "    model.eval()\n",
    "    out = []\n",
    "    labels = []\n",
    "    with torch.no_grad():\n",
    "      for i in range(0, len(features), 32):\n",
    "        inputs = features[i:i+32].cuda()\n",
    "        outputs = model(inputs)\n",
    "        # print(outputs)\n",
    "        # outputs.append(outputs.cpu().numpy())\n",
    "        out.append(outputs.cpu().numpy())\n",
    "    return out\n",
    "\n",
    "class BinaryClassifier(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(BinaryClassifier, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "def train_binary_classifier(model, D_r_features, D_r_labels, D_test_features, D_test_labels):\n",
    "    # Create binary labels for binary classification\n",
    "    D_r_binary_labels = np.ones_like(D_r_labels)\n",
    "    D_test_binary_labels = np.zeros_like(D_test_labels)\n",
    "\n",
    "    # Use model outputs on D_r as features for training\n",
    "    # binary_train_features = D_r_features\n",
    "    # binary_train_labels = D_r_binary_labels\n",
    "\n",
    "    binary_train_features = np.vstack((D_r_features, D_test_features))\n",
    "    binary_train_labels = np.concatenate((D_r_binary_labels, D_test_binary_labels)).flatten()\n",
    "\n",
    "    # Move binary classifier and data to GPU\n",
    "    binary_train_features_tensor = torch.tensor(binary_train_features, dtype=torch.float32).cuda()\n",
    "    binary_train_labels_tensor = torch.tensor(binary_train_labels, dtype=torch.float32).cuda()\n",
    "\n",
    "    # Initialize binary classifier\n",
    "    binary_classifier = BinaryClassifier(binary_train_features.shape[1]).cuda()\n",
    "\n",
    "    # Define binary cross-entropy loss and optimizer\n",
    "    binary_criterion = nn.BCEWithLogitsLoss()\n",
    "    binary_optimizer = optim.SGD(binary_classifier.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "    # Training loop for binary classifier\n",
    "    num_binary_epochs = 50  # You can adjust the number of epochs\n",
    "    for epoch in range(num_binary_epochs):\n",
    "        binary_classifier.train()\n",
    "        binary_optimizer.zero_grad()\n",
    "        binary_outputs = binary_classifier(binary_train_features_tensor)\n",
    "        binary_loss = binary_criterion(binary_outputs.squeeze(dim=1), binary_train_labels_tensor)\n",
    "        binary_loss.backward()\n",
    "        binary_optimizer.step()\n",
    "\n",
    "    return binary_classifier, binary_train_features_tensor, binary_train_labels_tensor"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wVDM5gRt9Poh",
    "outputId": "05d7dbba-9e3c-4bad-9630-d7b88a8ec42e"
   },
   "source": [
    "# for i in range(0, len(forget_features)):\n",
    "forget_inputs = forget_features.cuda()\n",
    "pred_forget_labels = forget_labels.cuda()\n",
    "        # labels_onehot = torch.zeros(labels.size(0), num_classes).cuda().scatter_(1, labels.view(-1, 1), 1)\n",
    "forget_labels_onehot = one_hot_encode(forget_labels.cuda(), num_classes)\n",
    "forget_outputs = classifier(forget_inputs)\n",
    "# loss_f = quadratic_loss(forget_outputs, forget_labels_onehot, classifier, mu) + psi* additional_regularization(b, classifier.fc.weight)\n",
    "loss_f = quadratic_loss(forget_outputs, forget_labels_onehot)\n",
    "print(loss_f)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tp-D5mErjy4k",
    "outputId": "cd89de27-1e61-433e-c504-2131a4acd80d"
   },
   "source": [
    "classifier.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(test_features), 32):\n",
    "        inputs = test_features[i:i+32].cuda()\n",
    "        labels = test_labels[i:i+32].cuda()\n",
    "\n",
    "        outputs = classifier(inputs)\n",
    "        _, predicted = outputs.max(1)\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "print('Accuracy on the test set using original model: %d %%' % (100 * correct / total))\n",
    "\n",
    "\n",
    "classifier.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(remaining_features), 32):\n",
    "        inputs = remaining_features[i:i+32].cuda()\n",
    "        labels = remaining_labels[i:i+32].cuda()\n",
    "\n",
    "        outputs = classifier(inputs)\n",
    "        _, predicted = outputs.max(1)\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "print('Accuracy on the remaining train set using original model: %d %%' % (100 * correct / total))\n",
    "\n",
    "\n",
    "classifier.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(forget_features), 32):\n",
    "        inputs = forget_features[i:i+32].cuda()\n",
    "        labels = forget_labels[i:i+32].cuda()\n",
    "\n",
    "        outputs = classifier(inputs)\n",
    "        _, predicted = outputs.max(1)\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "print('Accuracy on the forget train set using original model: %d %%' % (100 * correct / total))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aVQeGIjUwLTd",
    "outputId": "5dde5547-9810-4133-f943-66037e7e5471"
   },
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import random\n",
    "\n",
    "def cm_score(estimator, X, y):\n",
    "    y_pred = estimator.predict(X)\n",
    "    cnf_matrix = confusion_matrix(y, y_pred)\n",
    "\n",
    "    FP = cnf_matrix[0][1]\n",
    "    FN = cnf_matrix[1][0]\n",
    "    TP = cnf_matrix[0][0]\n",
    "    TN = cnf_matrix[1][1]\n",
    "\n",
    "\n",
    "    # Sensitivity, hit rate, recall, or true positive rate\n",
    "    TPR = TP/(TP+FN)\n",
    "    # Specificity or true negative rate\n",
    "    TNR = TN/(TN+FP)\n",
    "    # Precision or positive predictive value\n",
    "    PPV = TP/(TP+FP)\n",
    "    # Negative predictive value\n",
    "    NPV = TN/(TN+FN)\n",
    "    # Fall out or false positive rate\n",
    "    FPR = FP/(FP+TN)\n",
    "    # False negative rate\n",
    "    FNR = FN/(TP+FN)\n",
    "    # False discovery rate\n",
    "    FDR = FP/(TP+FP)\n",
    "\n",
    "    # Overall accuracy\n",
    "    ACC = (TP+TN)/(TP+FP+FN+TN)\n",
    "    print (f\"FPR:{FPR:.2f}, FNR:{FNR:.2f}, FP{FP:.2f}, TN{TN:.2f}, TP{TP:.2f}, FN{FN:.2f}\")\n",
    "    return ACC\n",
    "\n",
    "def evaluate_attack_model(sample_loss,\n",
    "                          members,\n",
    "                          n_splits = 100,\n",
    "                          random_state = None):\n",
    "  \"\"\"Computes the cross-validation score of a membership inference attack.\n",
    "  Args:\n",
    "    sample_loss : array_like of shape (n,).\n",
    "      objective function evaluated on n samples.\n",
    "    members : array_like of shape (n,),\n",
    "      whether a sample was used for training.\n",
    "    n_splits: int\n",
    "      number of splits to use in the cross-validation.\n",
    "    random_state: int, RandomState instance or None, default=None\n",
    "      random state to use in cross-validation splitting.\n",
    "  Returns:\n",
    "    score : array_like of size (n_splits,)\n",
    "  \"\"\"\n",
    "\n",
    "  unique_members = np.unique(members)\n",
    "  if not np.all(unique_members == np.array([0, 1])):\n",
    "    raise ValueError(\"members should only have 0 and 1s\")\n",
    "\n",
    "  attack_model = LogisticRegression()\n",
    "  cv = StratifiedShuffleSplit(\n",
    "      n_splits=n_splits, random_state=random_state)\n",
    "  return cross_val_score(attack_model, sample_loss, members, cv=cv, scoring=cm_score)\n",
    "\n",
    "\n",
    "def membership_inference_attack(model, test_f, test_l, forget_f, forget_l, seed):\n",
    "\n",
    "  fgt_cls = list(np.unique(forget_l))\n",
    "  indices = [i in fgt_cls for i in test_l]\n",
    "  test_f = test_f[indices]\n",
    "  test_l = test_l[indices]\n",
    "  cr = nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "  test_losses = []\n",
    "  forget_losses = []\n",
    "  bs=128\n",
    "  with torch.no_grad():\n",
    "    for i in range(0, len(test_f), bs):\n",
    "        inputs = test_f[i:i+bs].cuda()\n",
    "        labels = test_l[i:i+bs].cuda()\n",
    "        labels_onehot = one_hot_encode(labels.cuda(), 10)\n",
    "        outputs = model(inputs)\n",
    "        loss = cr(outputs, labels_onehot)\n",
    "        # print(loss)\n",
    "        # loss = quadratic_loss(outputs, labels_onehot)\n",
    "        test_losses = test_losses + list(loss.cpu().detach().numpy())\n",
    "        # test_losses.append(loss.cpu().detach().numpy())\n",
    "    for i in range(0, len(forget_f), bs):\n",
    "        inputs = forget_f[i:i+bs].cuda()\n",
    "        labels = forget_l[i:i+bs].cuda()\n",
    "        labels_onehot = one_hot_encode(labels.cuda(), 10)\n",
    "        outputs = model(inputs)\n",
    "        loss = cr(outputs, labels_onehot)\n",
    "        # print(loss)\n",
    "        # loss = quadratic_loss(outputs, labels_onehot)\n",
    "        forget_losses = forget_losses + list(loss.cpu().detach().numpy())\n",
    "        # forget_losses.append(loss.cpu().detach().numpy())\n",
    "\n",
    "  np.random.seed(seed)\n",
    "  random.seed(seed)\n",
    "  if len(forget_losses) > len(test_losses):\n",
    "      forget_losses = list(random.sample(forget_losses, len(test_losses)))\n",
    "  elif len(test_losses) > len(forget_losses):\n",
    "      test_losses = list(random.sample(test_losses, len(forget_losses)))\n",
    "\n",
    "  t_labels = [0]*len(test_losses)\n",
    "  f_labels = [1]*len(forget_losses)\n",
    "  features = np.array(test_losses + forget_losses).reshape(-1,1)\n",
    "  labels = np.array(t_labels + f_labels).reshape(-1)\n",
    "  # features = np.clip(features, -100, 100)\n",
    "  score = evaluate_attack_model(features, labels, n_splits=5, random_state=seed)\n",
    "\n",
    "  return score\n",
    "\n",
    "\n",
    "\n",
    "score = membership_inference_attack(classifier, test_features, test_labels, forget_features, forget_labels, 2023)\n",
    "\n",
    "print(np.mean(score))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A5s_OnxiKo2V"
   },
   "source": [
    "import numpy as np\n",
    "\n",
    "model_MI = classifier\n",
    "\n",
    "remaining_activations = extract_features(model_MI, remaining_features)\n",
    "remaining_activations = np.vstack(remaining_activations)\n",
    "\n",
    "test_activations = extract_features(model_MI, test_features)\n",
    "test_activations = np.vstack(test_activations)\n",
    "\n",
    "forget_activations = extract_features(model_MI, forget_features)\n",
    "forget_activations = np.vstack(forget_activations)\n",
    "\n",
    "train_activations = extract_features(model_MI, train_features)\n",
    "train_activations = np.vstack(train_activations)\n",
    "\n",
    "\n",
    "# binary_classifier, binary_train_features_tensor, binary_train_labels_tensor = train_binary_classifier(model_MI, remaining_activations, remaining_labels, test_activations, test_labels)\n",
    "# binary_classifier, binary_train_features_tensor, binary_train_labels_tensor = train_binary_classifier(model_MI, forget_activations, forget_labels, test_activations, test_labels)\n",
    "\n",
    "binary_classifier, binary_train_features_tensor, binary_train_labels_tensor = train_binary_classifier(model_MI, train_activations, train_labels, test_activations, test_labels)\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "binary_model = svm.SVC(kernel='linear')\n",
    "\n",
    "binary_model.fit(binary_train_features_tensor.cpu(), binary_train_labels_tensor.cpu())\n",
    "\n",
    "binary_test_features_tensor = torch.tensor(forget_activations, dtype=torch.float32).cuda()\n",
    "binary_test_outputs = binary_classifier(binary_test_features_tensor)\n",
    "# binary_test_outputs = binary_model.predict(binary_test_features_tensor.cpu())\n",
    "binary_predictions = (torch.sigmoid(torch.Tensor(binary_test_outputs)) > 0.5).cpu().numpy().astype(np.int)\n",
    "\n",
    "# Evaluate the results\n",
    "accuracy_binary = np.mean(binary_predictions == 1)  # Assuming D_f is correctly classified as 1\n",
    "# accuracy_binary = np.mean(binary_test_outputs == 1)\n",
    "print(f\"Original attack success on D_f: {accuracy_binary}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nrl8XOsTpvEu"
   },
   "source": [
    "print(binary_train_labels_tensor)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8GB6lDYi8B7k",
    "outputId": "356492ff-2a53-4e93-f510-04ba1df09ea9"
   },
   "source": [
    "\n",
    "\n",
    "for epoch in range(20):\n",
    "    for i in range(0, len(remaining_features), 32):\n",
    "        inputs = remaining_features[i:i+32].cuda()\n",
    "        r_labels = remaining_labels[i:i+32].cuda()\n",
    "        # labels_onehot = torch.zeros(r_labels.size(0), num_classes).cuda().scatter_(1, r_labels.view(-1, 1), 1)\n",
    "        labels_onehot_r = one_hot_encode(r_labels.cuda(), num_classes)\n",
    "\n",
    "        outputs = classifier_r(inputs)\n",
    "        # loss = quadratic_loss(outputs, labels_onehot_r, classifier, mu) + psi* additional_regularization(b, classifier.fc.weight)\n",
    "        loss = quadratic_loss(outputs, labels_onehot_r)\n",
    "\n",
    "        optimizer_r.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_r.step()\n",
    "\n",
    "        if i % 3200 == 0:  # Print training loss every 100 batches\n",
    "            print(f\"Epoch {epoch + 1}, Batch {i // 32 + 1}, Loss: {loss.item()}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vntLDsDl_I_a",
    "outputId": "0596e19f-8b67-419c-b744-1f5b3b3122f9"
   },
   "source": [
    "classifier_r.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(test_features), 32):\n",
    "        inputs = test_features[i:i+32].cuda()\n",
    "        labels = test_labels[i:i+32].cuda()\n",
    "\n",
    "        outputs = classifier_r(inputs)\n",
    "        _, predicted = outputs.max(1)\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "print('Accuracy on the test set using remaining set retrain model: %d %%' % (100 * correct / total))\n",
    "\n",
    "\n",
    "classifier_r.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(remaining_features), 32):\n",
    "        inputs = remaining_features[i:i+32].cuda()\n",
    "        labels = remaining_labels[i:i+32].cuda()\n",
    "\n",
    "        outputs = classifier_r(inputs)\n",
    "        _, predicted = outputs.max(1)\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "print('Accuracy on the remaining train set using remaining set retrain model: %d %%' % (100 * correct / total))\n",
    "\n",
    "\n",
    "classifier_r.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(forget_features), 32):\n",
    "        inputs = forget_features[i:i+32].cuda()\n",
    "        labels = forget_labels[i:i+32].cuda()\n",
    "\n",
    "        outputs = classifier_r(inputs)\n",
    "        _, predicted = outputs.max(1)\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "print('Accuracy on the forget train set using remaining set retrain model: %d %%' % (100 * correct / total))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VaNsFGwfFc3P",
    "outputId": "8f710929-fceb-453a-d7d4-058afbd83cfc"
   },
   "source": [
    "score = membership_inference_attack(classifier_r, test_features, test_labels, forget_features, forget_labels, 2023)\n",
    "\n",
    "print(np.mean(score))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6wECPQdcLIwg"
   },
   "source": [
    "model_MI = classifier_r\n",
    "\n",
    "# remaining_activations = extract_features(model_MI, remaining_features)\n",
    "# remaining_activations = np.vstack(remaining_activations)\n",
    "\n",
    "# test_activations = extract_features(model_MI, test_features)\n",
    "# test_activations = np.vstack(test_activations)\n",
    "\n",
    "forget_activations = extract_features(model_MI, forget_features)\n",
    "forget_activations = np.vstack(forget_activations)\n",
    "\n",
    "\n",
    "# binary_classifier = train_binary_classifier(model_MI, remaining_activations, remaining_labels, test_activations, test_labels)\n",
    "\n",
    "binary_test_features_tensor = torch.tensor(forget_activations, dtype=torch.float32).cuda()\n",
    "binary_test_outputs = binary_classifier(binary_test_features_tensor)\n",
    "# binary_test_outputs = binary_model.predict(binary_test_features_tensor.cpu())\n",
    "binary_predictions = (torch.sigmoid(torch.Tensor(binary_test_outputs)) > 0.5).cpu().numpy().astype(np.int)\n",
    "\n",
    "# Evaluate the results\n",
    "accuracy_binary = np.mean(binary_predictions == 1)  # Assuming D_f is correctly classified as 1\n",
    "# accuracy_binary = np.mean(binary_test_outputs == 1)\n",
    "print(f\"retrain attack success on D_f: {accuracy_binary}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X3qTgbmZ_qPm",
    "outputId": "d72048b5-3618-45d9-8bef-e873da00d7bb"
   },
   "source": [
    "for name, param in classifier.named_parameters():\n",
    "  print(name)\n",
    "  if name == 'fc.weight':\n",
    "    W = param.data\n",
    "  # else:\n",
    "  #   b = param\n",
    "  print(f'Parameter {name}, shape {param.shape}')\n",
    "  print(param.data.size())\n",
    "\n",
    "# W = param.data.T\n",
    "print(W.shape)\n",
    "print(type(W))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mIalQOyBApAa",
    "outputId": "42cbec4b-94e0-4cd6-a6e9-8bb82ed49b16"
   },
   "source": [
    "H_r = remaining_features.T@remaining_features/len(train_features)\n",
    "# H_r = remaining_features.T@remaining_features/len(remaining_features)\n",
    "H_f = forget_features.T@forget_features/len(train_features)\n",
    "# H_f = forget_features.T@forget_features/len(forget_features)\n",
    "H = train_features.T@train_features/len(train_features)\n",
    "print(H_r)\n",
    "print(H_f)\n",
    "print(H)\n",
    "# print(H_f)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H66EASqNK0QZ",
    "outputId": "5c91f64d-f12c-442c-8223-26afaf479529"
   },
   "source": [
    "# forget_labels_onehot = torch.zeros(forget_labels.size(0), num_classes).cuda().scatter_(1, labels.view(-1, 1), 1)\n",
    "forget_labels_onehot = one_hot_encode(forget_labels.cuda(), num_classes)\n",
    "\n",
    "\n",
    "# if len(forget_labels.shape) == 1:\n",
    "#    forget_labels = forget_labels.unsqueeze(1)\n",
    "\n",
    "# forget_labels_onehot = torch.zeros(forget_labels.size(0), num_classes)\n",
    "# forget_labels_onehot.cuda().scatter_(1, forget_labels.cuda(), 1)\n",
    "\n",
    "print(forget_labels[16])\n",
    "print(forget_labels_onehot[16])\n",
    "\n",
    "forget_features = torch.tensor(forget_features).cuda()\n",
    "print(type(forget_features))\n",
    "print(type(forget_labels_onehot))\n",
    "print(type(W))\n",
    "grad_f = (forget_features.T@(forget_features@W.T-forget_labels_onehot.cuda())/len(train_features))\n",
    "# grad_f = (forget_features.T@(forget_features@W.T-forget_labels_onehot.cuda())/len(forget_features))\n",
    "print(grad_f.shape)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pAjy7zxCNN6E",
    "outputId": "2307fbcc-3aaf-429f-f63a-e00673a58d9f"
   },
   "source": [
    "grad_f = torch.tensor(grad_f)\n",
    "hess_grad = torch.linalg.inv(H_r).cuda()@grad_f\n",
    "\n",
    "# hess_grad = len(remaining_features)*torch.linalg.inv(H).cuda()@grad_f/len(train_features)\n",
    "# hess_grad = torch.linalg.inv(H).cuda()@grad_f\n",
    "W_new = W.T+hess_grad.cuda()\n",
    "# print(W_new)\n",
    "pretrained_dict = classifier.state_dict()\n",
    "pretrained_dict['fc.weight'] = W_new.T\n",
    "classifier.load_state_dict(pretrained_dict)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5kQ-QVa1OZVB",
    "outputId": "cb5d9fc7-224d-4c71-9b36-4b270dfc9655"
   },
   "source": [
    "classifier.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(test_features), 32):\n",
    "        inputs = test_features[i:i+32].cuda()\n",
    "        labels = test_labels[i:i+32].cuda()\n",
    "\n",
    "        outputs = classifier(inputs)\n",
    "        _, predicted = outputs.max(1)\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "print('Accuracy on the test set using actual scrubbed model: %d %%' % (100 * correct / total))\n",
    "\n",
    "\n",
    "classifier.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(remaining_features), 32):\n",
    "        inputs = remaining_features[i:i+32].cuda()\n",
    "        labels = remaining_labels[i:i+32].cuda()\n",
    "\n",
    "        outputs = classifier(inputs)\n",
    "        _, predicted = outputs.max(1)\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "print('Accuracy on the remaining train set using actual scrubbed model: %d %%' % (100 * correct / total))\n",
    "\n",
    "\n",
    "classifier.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(forget_features), 32):\n",
    "        inputs = forget_features[i:i+32].cuda()\n",
    "        labels = forget_labels[i:i+32].cuda()\n",
    "\n",
    "        outputs = classifier(inputs)\n",
    "        _, predicted = outputs.max(1)\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "print('Accuracy on the forget train set using actual scrubbed model: %d %%' % (100 * correct / total))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s_2M_GQGFvSP",
    "outputId": "3cce9474-7a2a-4886-eddd-5258b1d754c3"
   },
   "source": [
    "score = membership_inference_attack(classifier, test_features, test_labels, forget_features, forget_labels, 2023)\n",
    "\n",
    "print(np.mean(score))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 201
    },
    "id": "p2ZXKvfhLNl4",
    "outputId": "6788893a-407d-4d0b-e95b-ec4f0df1f426"
   },
   "source": [
    "model_MI = classifier\n",
    "\n",
    "# remaining_activations = extract_features(model_MI, remaining_features)\n",
    "# remaining_activations = np.vstack(remaining_activations)\n",
    "\n",
    "# test_activations = extract_features(model_MI, test_features)\n",
    "# test_activations = np.vstack(test_activations)\n",
    "\n",
    "forget_activations = extract_features(model_MI, forget_features)\n",
    "forget_activations = np.vstack(forget_activations)\n",
    "\n",
    "\n",
    "# binary_classifier = train_binary_classifier(model_MI, remaining_activations, remaining_labels, test_activations, test_labels)\n",
    "\n",
    "binary_test_features_tensor = torch.tensor(forget_activations, dtype=torch.float32).cuda()\n",
    "binary_test_outputs = binary_classifier(binary_test_features_tensor)\n",
    "# binary_test_outputs = binary_model.predict(binary_test_features_tensor.cpu())\n",
    "binary_predictions = (torch.sigmoid(torch.Tensor(binary_test_outputs)) > 0.5).cpu().numpy().astype(np.int)\n",
    "\n",
    "# Evaluate the results\n",
    "accuracy_binary = np.mean(binary_predictions == 1)  # Assuming D_f is correctly classified as 1\n",
    "# accuracy_binary = np.mean(binary_test_outputs == 1)\n",
    "print(f\"Original scrubbed attack success on D_f: {accuracy_binary}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9B1jjEI3PHQW",
    "outputId": "8cd5cb38-5d45-4ca4-93a1-d8ec06f5b658"
   },
   "source": [
    "# if len(train_labels.shape) == 1:\n",
    "#     train_labels = train_labels.unsqueeze(1)\n",
    "\n",
    "# train_labels_onehot = torch.zeros(train_labels.size(0), num_classes)\n",
    "# train_labels_onehot.scatter_(1, train_labels, 1)\n",
    "train_labels_onehot = one_hot_encode(train_labels.cuda(), num_classes)\n",
    "# print(one_hot.shape)  # torch.Size([50000, 10])\n",
    "# train_labels_onehot = torch.zeros(train_labels.size(0), 10).cuda().scatter_(1, labels.view(-1, 1), 1)\n",
    "print(train_labels.shape)\n",
    "Y_train = train_labels_onehot.T@train_labels_onehot/len(train_features)\n",
    "Y = Y_train.cpu().numpy()\n",
    "print(Y_train)\n",
    "W = W.cpu().numpy()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Z-UHHRcQKKi"
   },
   "source": [
    "# import cvxpy as cp\n",
    "# import numpy as np\n",
    "\n",
    "# # Create constant matrices\n",
    "# # C = np.array([[2, -1], [-1, 3]])\n",
    "# # A1 = np.array([[1, 0], [0, 0]])\n",
    "# # A2 = np.array([[0, 0], [0, 1]])\n",
    "# d=512\n",
    "# lambda_max_value = d\n",
    "# # Define variables\n",
    "# X = cp.Variable((d, d), symmetric=True)\n",
    "\n",
    "# # Define constraints\n",
    "# constraints = [X >> 0]\n",
    "# constraints += [X >> H_f.cpu().numpy()]\n",
    "# # constraints += [X - lambda_max_value * torch.eye(d).numpy() << 0]\n",
    "# # constraints +=[cp.trace(X) <= 1]\n",
    "# constraints +=[cp.trace(X) >= 0]\n",
    "# # constraints += [X[i, i] == 1 for i in range(d)]\n",
    "# #constraints += [cp.trace(w@X.T@X@w.T) == len(y_train)]\n",
    "\n",
    "# # Define objective\n",
    "# # objective = cp.Minimize(cp.trace((w@X@w.T-Y_train)))\n",
    "\n",
    "# objective = cp.Minimize(cp.trace((W@X@W.T/len(train_features)-Y)))\n",
    "# # objective = cp.Minimize(cp.trace((Y_train-w@X.T@y_train)))\n",
    "\n",
    "# # objective = cp.Minimize(cp.norm(W@X@W.T/len(train_dataset)-Y, 'fro')**2)\n",
    "\n",
    "# # Define problem\n",
    "# problem = cp.Problem(objective, constraints)\n",
    "\n",
    "# # Solve problem\n",
    "# problem.solve(qcp = True)\n",
    "# # problem.solve()\n",
    "\n",
    "# print(\"Optimal value: \", problem.value)\n",
    "# print(\"Optimal variable X: \")\n",
    "# # print(X.value)\n",
    "# err = X.value-H_f.cpu().numpy()-H_r.cpu().numpy()\n",
    "# # print(X.value)\n",
    "# print(np.sqrt(np.trace(np.dot(err.T,err)))/d)\n",
    "\n",
    "\n",
    "\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TgjYHa54GzdT",
    "outputId": "b88e5f73-91af-465c-fa58-16df35a181d5"
   },
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import cvxpy as cp\n",
    "# import numpy as np\n",
    "\n",
    "true_hessian = H\n",
    "dim = 512\n",
    "# Perturbations and observed gradient changes\n",
    "num_perturbations = 500\n",
    "delta_w = torch.randn(num_perturbations, dim) * 0.01  # Small perturbations\n",
    "delta_L = []\n",
    "\n",
    "# optimal_w = next(model.parameters()).detach().clone()\n",
    "for dw in delta_w:\n",
    "    # print(W.shape)\n",
    "    # dw[np.newaxis, :]\n",
    "    dw = np.reshape(dw, (1, 512))\n",
    "    # print(dw.shape)\n",
    "    W_noisy = W + dw.numpy()\n",
    "    # print(W_noisy.shape)\n",
    "    W_noisy = torch.tensor(W_noisy)\n",
    "    # pretrained_dict = model.state_dict()\n",
    "    # pretrained_dict['fc.weight'] = W_noisy\n",
    "    # model.load_state_dict(pretrained_dict)\n",
    "    classifier.fc.weight.data =  W_noisy.cuda()\n",
    "    # outputs = model(X_train)\n",
    "    # loss_perturbed = criterion(outputs, y_train.float())\n",
    "    outputs = classifier(forget_features)\n",
    "    # print(outputs.shape)\n",
    "    # print(forget_labels.shape)\n",
    "    # loss_perturbed = quadratic_loss(outputs, forget_labels_onehot, classifier, mu) + psi* additional_regularization(b, classifier.fc.weight)\n",
    "    loss_perturbed = quadratic_loss(outputs, forget_labels_onehot)\n",
    "    # delta_L.append(loss_perturbed.item() - loss.item())\n",
    "    delta_L.append(loss_perturbed.item() - loss_f.item())\n",
    "\n",
    "delta_L = np.array(delta_L)\n",
    "delta_L = delta_L.reshape(-1, 1)\n",
    "\n",
    "# CVXPY problem to estimate Hessian\n",
    "d = dim\n",
    "X = cp.Variable((d, d), symmetric=True)\n",
    "\n",
    "# Create a list to hold our quadratic forms for each perturbation\n",
    "delta_w_matrix = np.stack([dw.numpy() for dw in delta_w])\n",
    "\n",
    "# Calculate the quadratic forms more efficiently\n",
    "quadratic_forms_vectorized = 0.5 * cp.sum(cp.multiply(delta_w_matrix @ X, delta_w_matrix), axis=1)\n",
    "\n",
    "objective = cp.Minimize(cp.sum_squares(delta_L.flatten() - quadratic_forms_vectorized))\n",
    "\n",
    "# Assuming the same constraints\n",
    "# constraints = [X >> 0, cp.trace(X) <= 1, cp.trace(X) >= 0, X >> H_f]\n",
    "constraints = [X >> 0, cp.trace(X) >= 0, X >> H_f]\n",
    "# constraints += [H[i, i] <= 0.25 for i in range(d)]\n",
    "prob = cp.Problem(objective, constraints)\n",
    "prob.solve()\n",
    "\n",
    "# Estimated Hessian\n",
    "H_value = X.value\n",
    "\n",
    "# Compute difference between true and estimated Hessian\n",
    "hessian_diff = np.linalg.norm(H - H_value, 'fro')\n",
    "\n",
    "print(\"True Hessian:\")\n",
    "print(true_hessian)\n",
    "print(\"\\nEstimated Hessian:\")\n",
    "print(H_value)\n",
    "\n",
    "print(\"\\nDifference (Frobenius norm):\", hessian_diff/d**2)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HFsWTAr3PJoq",
    "outputId": "2e092262-202f-4bc6-e87b-90165ae0e9ef"
   },
   "source": [
    "print(X.value)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KBJCtcaFTe4K",
    "outputId": "773efc91-8426-4b72-8ea5-ca1d8bb0c52e"
   },
   "source": [
    "from torch.autograd import Variable\n",
    "# H_r_aprx = X.value/len(train_features) - H_f.cpu().numpy()\n",
    "H_r_aprx = X.value - H_f.cpu().numpy()\n",
    "H_r_aprx = Variable(torch.Tensor(H_r_aprx).float()).cuda()\n",
    "H_aprx = Variable(torch.Tensor(X.value).float()).cuda()\n",
    "hess_grad_aprx = torch.linalg.inv(H_r_aprx)@grad_f\n",
    "\n",
    "# hess_grad_aprx = len(remaining_features)*torch.linalg.inv(H_aprx)@grad_f/len(train_features)\n",
    "# hess_grad_aprx = torch.linalg.inv(H_aprx)@grad_f\n",
    "W = Variable(torch.Tensor(W).float()).cuda()\n",
    "print(type(hess_grad_aprx))\n",
    "print(type(W))\n",
    "W_new_aprx = W.T + hess_grad_aprx.cuda()\n",
    "print(type(W_new_aprx))\n",
    "pretrained_dict = classifier.state_dict()\n",
    "pretrained_dict['fc.weight'] = W_new_aprx.T\n",
    "classifier.load_state_dict(pretrained_dict)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qK2-QQIPT6G-",
    "outputId": "5013fae6-f3a7-4f27-aa49-fdb0e8ae6082"
   },
   "source": [
    "classifier.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(test_features), 32):\n",
    "        inputs = test_features[i:i+32].cuda()\n",
    "        labels = test_labels[i:i+32].cuda()\n",
    "\n",
    "        outputs = classifier(inputs)\n",
    "        _, predicted = outputs.max(1)\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "print('Accuracy on the test set using approximate scrubbed model: %d %%' % (100 * correct / total))\n",
    "\n",
    "\n",
    "classifier.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(remaining_features), 32):\n",
    "        inputs = remaining_features[i:i+32].cuda()\n",
    "        labels = remaining_labels[i:i+32].cuda()\n",
    "\n",
    "        outputs = classifier(inputs)\n",
    "        _, predicted = outputs.max(1)\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "print('Accuracy on the remaining train set using approximate scrubbed model: %d %%' % (100 * correct / total))\n",
    "\n",
    "\n",
    "classifier.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(forget_features), 32):\n",
    "        inputs = forget_features[i:i+32].cuda()\n",
    "        labels = forget_labels[i:i+32].cuda()\n",
    "\n",
    "        outputs = classifier(inputs)\n",
    "        _, predicted = outputs.max(1)\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "print('Accuracy on the forget train set using approximate scrubbed model: %d %%' % (100 * correct / total))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lmnrmm-eIeWe",
    "outputId": "88703a5c-91fb-4da9-b13a-056dd8194985"
   },
   "source": [
    "score = membership_inference_attack(classifier, test_features, test_labels, forget_features, forget_labels, 2023)\n",
    "\n",
    "print(np.mean(score))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 201
    },
    "id": "73yvKpQILVa7",
    "outputId": "f3564ebf-687e-4464-fde7-13c8d99e6d0c"
   },
   "source": [
    "model_MI = classifier\n",
    "\n",
    "# remaining_activations = extract_features(model_MI, remaining_features)\n",
    "# remaining_activations = np.vstack(remaining_activations)\n",
    "\n",
    "# test_activations = extract_features(model_MI, test_features)\n",
    "# test_activations = np.vstack(test_activations)\n",
    "\n",
    "forget_activations = extract_features(model_MI, forget_features)\n",
    "forget_activations = np.vstack(forget_activations)\n",
    "\n",
    "\n",
    "# binary_classifier = train_binary_classifier(model_MI, remaining_activations, remaining_labels, test_activations, test_labels)\n",
    "\n",
    "binary_test_features_tensor = torch.tensor(forget_activations, dtype=torch.float32).cuda()\n",
    "binary_test_outputs = binary_classifier(binary_test_features_tensor)\n",
    "# binary_test_outputs = binary_model.predict(binary_test_features_tensor.cpu())\n",
    "binary_predictions = (torch.sigmoid(torch.tensor(binary_test_outputs)) > 0.5).cpu().numpy().astype(np.int)\n",
    "\n",
    "# Evaluate the results\n",
    "accuracy_binary = np.mean(binary_predictions == 1)  # Assuming D_f is correctly classified as 1\n",
    "# accuracy_binary = np.mean(binary_test_outputs == 1)\n",
    "print(f\"Approx scrubbed attack success on D_f: {accuracy_binary}\")"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
